{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd4c75f-7810-4c13-8f82-508da0c01cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers numpy psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6b38ca-d974-43ad-a228-4eaf085d75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda6f6b4-9de1-4975-a565-881ee8376a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 15 20:14:40 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   26C    P8    25W / 250W |      0MiB / 22698MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d14b263-4a2d-419b-8d75-9f9e18e5d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "\u001b[33mDEPRECATION: celery 4.1.0 has a non-standard dependency specifier pytz>dev. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba1fb8f-5b26-4742-926d-58805bc09812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load HF token from .env\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ca15f4-caa1-420d-9367-6b7502088362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, asdict, field\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "import statistics\n",
    "import threading\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "@dataclass\n",
    "class HardwareConfig:\n",
    "    gpu_mem_total: int = 22698  # RTX 6000 memory in MB\n",
    "    cpu_count: int = 8\n",
    "    cpu_mem_total: int = 1024 * 1024  # Keep as is unless you want to adjust\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    model_name: str = \"meta-llama/Llama-3.2-1B\"\n",
    "    context_lengths: List[int] = field(default_factory=lambda: [512, 1024, 2048])\n",
    "    output_lengths: List[int] = field(default_factory=lambda: [32, 64])\n",
    "    batch_size: int = 1\n",
    "    num_runs: int = 3\n",
    "    warmup_runs: int = 1\n",
    "    output_dir: str = \"benchmark_results\"\n",
    "    hardware: HardwareConfig = field(default_factory=HardwareConfig)\n",
    "    decode_strategy: str = \"gpu\"\n",
    "\n",
    "# Add memory optimization to model loading\n",
    "def load_model(self):\n",
    "    \"\"\"Load model to GPU with memory optimizations\"\"\"\n",
    "    self.logger.info(f\"Loading model: {self.config.model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Use mixed precision\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.model_name,\n",
    "        )\n",
    "        \n",
    "        model_size = sum(p.numel() for p in self.model.parameters()) * 2 / (1024**3)\n",
    "        self.logger.info(f\"Model size: {model_size:.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Failed to load model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self, sampling_rate: float = 0.1):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.cpu_percentages = []\n",
    "        self._stop_monitoring = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gpu_memory_usage():\n",
    "        if torch.cuda.is_available():\n",
    "            return {\n",
    "                'allocated': torch.cuda.memory_allocated() / 1024**2,\n",
    "                'reserved': torch.cuda.memory_reserved() / 1024**2,\n",
    "                'max_allocated': torch.cuda.max_memory_allocated() / 1024**2\n",
    "            }\n",
    "        return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cpu_memory_usage():\n",
    "        vm = psutil.virtual_memory()\n",
    "        return {\n",
    "            'total': vm.total / 1024**2,\n",
    "            'available': vm.available / 1024**2,\n",
    "            'used': vm.used / 1024**2,\n",
    "            'cached': getattr(vm, 'cached', 0) / 1024**2\n",
    "        }\n",
    "    \n",
    "    def _monitor_cpu(self):\n",
    "        while not self._stop_monitoring:\n",
    "            self.cpu_percentages.append(psutil.cpu_percent(percpu=True))\n",
    "            time.sleep(self.sampling_rate)\n",
    "    \n",
    "    @contextmanager\n",
    "    def track_resources(self):\n",
    "        \"\"\"Enhanced resource tracking with CPU utilization\"\"\"\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        self.cpu_percentages = []\n",
    "        self._stop_monitoring = False\n",
    "        resources = {}\n",
    "        \n",
    "        monitor_thread = threading.Thread(target=self._monitor_cpu)\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "        \n",
    "        try:\n",
    "            yield resources\n",
    "        finally:\n",
    "            self._stop_monitoring = True\n",
    "            monitor_thread.join(timeout=1.0)\n",
    "            \n",
    "            gpu_mem = self.get_gpu_memory_usage()\n",
    "            cpu_mem = self.get_cpu_memory_usage()\n",
    "            \n",
    "            if self.cpu_percentages:\n",
    "                cpu_stats = {\n",
    "                    'mean_per_core': [statistics.mean(core_vals) for core_vals in zip(*self.cpu_percentages)],\n",
    "                    'max_per_core': [max(core_vals) for core_vals in zip(*self.cpu_percentages)],\n",
    "                    'overall_mean': statistics.mean([sum(vals)/len(vals) for vals in self.cpu_percentages])\n",
    "                }\n",
    "            else:\n",
    "                cpu_stats = {\n",
    "                    'mean_per_core': [0],\n",
    "                    'max_per_core': [0],\n",
    "                    'overall_mean': 0\n",
    "                }\n",
    "            \n",
    "            resources.update({\n",
    "                'gpu': gpu_mem,\n",
    "                'cpu': cpu_mem,\n",
    "                'cpu_utilization': cpu_stats\n",
    "            })\n",
    "\n",
    "class ModelBenchmark:\n",
    "    def __init__(self, config: BenchmarkConfig):\n",
    "        self.config = config\n",
    "        self.logger = self._setup_logging()\n",
    "        self._setup_directories()\n",
    "        self.monitor = ResourceMonitor()\n",
    "        self.kv_cache = None        \n",
    "        self.current_sequence_length = 0\n",
    "        self.initial_sequence_length = 0\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        return logger\n",
    "\n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        Path(self.config.output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model to GPU initially\"\"\"\n",
    "        self.logger.info(f\"Loading model: {self.config.model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Always load to GPU first\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\"  # Start with everything on GPU\n",
    "            )\n",
    "            \n",
    "            # Debug: Log model architecture\n",
    "            self.logger.info(\"Model architecture:\")\n",
    "            for name, module in self.model.named_children():\n",
    "                self.logger.info(f\"- {name}: {type(module)}\")\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config.model_name,\n",
    "            )\n",
    "            \n",
    "            model_size = sum(p.numel() for p in self.model.parameters()) * 2 / (1024**3)\n",
    "            self.logger.info(f\"Model size: {model_size:.2f} GB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _log_state(self, phase: str):\n",
    "        \"\"\"Log current state for debugging\"\"\"\n",
    "        self.logger.info(f\"\\n=== State during {phase} ===\")\n",
    "        if self.kv_cache is not None:\n",
    "            # Log KV cache details\n",
    "            kv_first_layer = self.kv_cache[0]\n",
    "            k, v = kv_first_layer\n",
    "        #     self.logger.info(f\"KV cache first layer shapes: K={k.shape}, V={v.shape}\")\n",
    "        #     self.logger.info(f\"KV cache device: {k.device}\")\n",
    "        # self.logger.info(f\"Current sequence length: {self.current_sequence_length}\")\n",
    "        \n",
    "    def prefill_phase(self, input_ids):\n",
    "        \"\"\"Run prefill phase on GPU\"\"\"\n",
    "        # self.logger.info(\"Running prefill phase on GPU\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Store initial sequence length\n",
    "            self.initial_sequence_length = input_ids.shape[1]\n",
    "            self.current_sequence_length = self.initial_sequence_length\n",
    "            \n",
    "            self._log_state(\"before prefill\")\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids,\n",
    "                use_cache=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            self.kv_cache = outputs.past_key_values\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            self._log_state(\"after prefill\")\n",
    "            \n",
    "        return next_token_logits\n",
    "\n",
    "    def prepare_decode_strategy(self):\n",
    "        \"\"\"Prepare model for decode phase based on strategy\"\"\"\n",
    "        if self.config.decode_strategy == \"cpu\":\n",
    "            # self.logger.info(\"Moving decoder to CPU for decode phase\")\n",
    "            \n",
    "            # For OPT models, the decoder is at model.model.decoder\n",
    "            if hasattr(self.model, 'model') and hasattr(self.model.model, 'decoder'):\n",
    "                self.model.model.decoder.to(\"cpu\")\n",
    "                \n",
    "                if self.kv_cache is not None:\n",
    "                    # Move KV cache to CPU and log shapes before/after\n",
    "                    self._log_state(\"before moving KV cache\")\n",
    "                    self.kv_cache = tuple(\n",
    "                        tuple(t.to(\"cpu\") for t in layer)\n",
    "                        for layer in self.kv_cache\n",
    "                    )\n",
    "                    self._log_state(\"after moving KV cache\")\n",
    "            else:\n",
    "                self.logger.warning(\"Could not find decoder in expected location\")\n",
    "\n",
    "    def decode_step(self, input_ids, attention_mask=None):\n",
    "        \"\"\"Single decode step using stored KV cache\"\"\"\n",
    "        with torch.no_grad():\n",
    "            device = \"cpu\" if self.config.decode_strategy == \"cpu\" else \"cuda\"\n",
    "            \n",
    "            # Ensure input_ids has the right shape\n",
    "            if len(input_ids.shape) == 1:\n",
    "                input_ids = input_ids.unsqueeze(0)\n",
    "            if len(input_ids.shape) == 2 and input_ids.shape[1] != 1:\n",
    "                input_ids = input_ids[:, -1:]\n",
    "                \n",
    "            input_ids = input_ids.to(device)\n",
    "            \n",
    "            # Get sequence length from KV cache\n",
    "            k, v = self.kv_cache[0]  # First layer\n",
    "            past_seq_len = k.shape[2]  # seq_len dimension in KV cache\n",
    "            \n",
    "            # Create attention mask including both past and current token\n",
    "            batch_size = input_ids.shape[0]\n",
    "            total_seq_len = past_seq_len + 1  # Include current token\n",
    "            \n",
    "            full_attention_mask = torch.ones(\n",
    "                (batch_size, total_seq_len),\n",
    "                dtype=torch.long,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # self.logger.info(f\"Attention mask shape: {full_attention_mask.shape}\")\n",
    "            # self.logger.info(f\"Past sequence length: {past_seq_len}\")\n",
    "            # self.logger.info(f\"Total sequence length: {total_seq_len}\")\n",
    "            \n",
    "            self._log_state(\"before decode step\")\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=full_attention_mask,\n",
    "                use_cache=True,\n",
    "                past_key_values=self.kv_cache,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Update KV cache\n",
    "            self.kv_cache = outputs.past_key_values\n",
    "            self.current_sequence_length = total_seq_len\n",
    "            \n",
    "            self._log_state(\"after decode step\")\n",
    "            \n",
    "            return outputs.logits[:, -1, :]\n",
    "\n",
    "    def run_single_generation(self, tokens, output_length: int):\n",
    "        \"\"\"Run full generation with separate prefill and decode phases\"\"\"\n",
    "        with self.monitor.track_resources() as resources:\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Reset sequence tracking\n",
    "                self.current_sequence_length = 0\n",
    "                \n",
    "                # Prefill phase\n",
    "                input_ids = tokens.input_ids\n",
    "                next_token_logits = self.prefill_phase(input_ids)\n",
    "                \n",
    "                # Prepare decode strategy\n",
    "                self.prepare_decode_strategy()\n",
    "                \n",
    "                # Generate tokens\n",
    "                generated_tokens = []\n",
    "                for i in range(output_length):\n",
    "                    self.logger.info(f\"Generating token {i+1}/{output_length}\")\n",
    "                    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "                    generated_tokens.append(next_token)\n",
    "                    \n",
    "                    # Prepare input for next decode step\n",
    "                    current_input = next_token.unsqueeze(0)\n",
    "                    \n",
    "                    # Decode step\n",
    "                    next_token_logits = self.decode_step(current_input)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Move back to GPU for next run if needed\n",
    "            if self.config.decode_strategy == \"cpu\":\n",
    "                if hasattr(self.model, 'model') and hasattr(self.model.model, 'decoder'):\n",
    "                    self.model.model.decoder.to(\"cuda\")\n",
    "        \n",
    "        return {\n",
    "            'time': end_time - start_time,\n",
    "            'resources': resources,\n",
    "            'output_length': len(generated_tokens)\n",
    "        }\n",
    "        \n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"Clean up GPU memory between runs\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        gc.collect()\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        \"\"\"Run the complete benchmark suite\"\"\"\n",
    "        self.load_model()\n",
    "        results = []\n",
    "        \n",
    "        # Create sample input text if no document provided\n",
    "        sample_text = \"This is a sample text for benchmarking. \" * 100\n",
    "        \n",
    "        for context_length in self.config.context_lengths:\n",
    "            for output_length in self.config.output_lengths:\n",
    "                self.logger.info(f\"\\nTesting context length: {context_length}, output length: {output_length}\")\n",
    "                self.clean_memory()\n",
    "                \n",
    "                # Prepare input\n",
    "                tokens = self.tokenizer(\n",
    "                    sample_text,\n",
    "                    truncation=True,\n",
    "                    max_length=context_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(\"cuda\")\n",
    "                \n",
    "                # Warmup\n",
    "                self.logger.info(\"Performing warmup runs...\")\n",
    "                for _ in range(self.config.warmup_runs):\n",
    "                    _ = self.run_single_generation(tokens, output_length)\n",
    "                \n",
    "                # Benchmark runs\n",
    "                run_results = []\n",
    "                for run in range(self.config.num_runs):\n",
    "                    self.logger.info(f\"Run {run + 1}/{self.config.num_runs}\")\n",
    "                    try:\n",
    "                        result = self.run_single_generation(tokens, output_length)\n",
    "                        run_results.append(result)\n",
    "                        \n",
    "                        self.logger.info(f\"Generation time: {result['time']:.2f}s\")\n",
    "                        self.logger.info(f\"Tokens per second: {output_length/result['time']:.2f}\")\n",
    "                        \n",
    "                        cpu_util = result['resources']['cpu_utilization']['overall_mean']\n",
    "                        self.logger.info(f\"CPU utilization: {cpu_util:.1f}%\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                if not run_results:\n",
    "                    continue\n",
    "                    \n",
    "                # Aggregate results\n",
    "                result = {\n",
    "                    'context_length': context_length,\n",
    "                    'output_length': output_length,\n",
    "                    'avg_time': statistics.mean([r['time'] for r in run_results]),\n",
    "                    'std_time': statistics.stdev([r['time'] for r in run_results]) if len(run_results) > 1 else 0,\n",
    "                    'tokens_per_second': output_length / statistics.mean([r['time'] for r in run_results]),\n",
    "                    'gpu_memory_peak': max([r['resources']['gpu']['max_allocated'] for r in run_results]),\n",
    "                    'cpu_utilization': {\n",
    "                        'mean': statistics.mean([r['resources']['cpu_utilization']['overall_mean'] for r in run_results]),\n",
    "                        'peak': max([max(r['resources']['cpu_utilization']['max_per_core']) for r in run_results])\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06aa5a6a-2684-4c71-b02c-89560b412799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 20:17:31,074 - __main__ - INFO - Loading model: meta-llama/Llama-3.2-1B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8927e9ea4841238778e29c3839867b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556557ea92844294a8e1f81e20df055f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 20:18:41,435 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f229678b224bc2b8f2a6445323e330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 20:18:43,089 - __main__ - INFO - Model architecture:\n",
      "2024-12-15 20:18:43,090 - __main__ - INFO - - model: <class 'transformers.models.llama.modeling_llama.LlamaModel'>\n",
      "2024-12-15 20:18:43,090 - __main__ - INFO - - lm_head: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a1aeb29af247c9ab36b7ac9c511949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33875c5bdb8473498e20b55e6c53811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0aca4c57c7d4643ad22e4d227bf0c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 20:18:44,447 - __main__ - INFO - Model size: 2.30 GB\n",
      "2024-12-15 20:18:44,448 - __main__ - INFO - \n",
      "Testing context length: 256, output length: 16\n",
      "2024-12-15 20:18:44,581 - __main__ - INFO - Performing warmup runs...\n",
      "2024-12-15 20:18:44,583 - __main__ - INFO - \n",
      "=== State during before prefill ===\n",
      "2024-12-15 20:18:45,118 - __main__ - INFO - \n",
      "=== State during after prefill ===\n",
      "2024-12-15 20:18:45,119 - __main__ - INFO - Generating token 1/16\n",
      "2024-12-15 20:18:45,120 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "2024-12-15 20:18:45,237 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,238 - __main__ - INFO - Generating token 2/16\n",
      "2024-12-15 20:18:45,239 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,255 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,256 - __main__ - INFO - Generating token 3/16\n",
      "2024-12-15 20:18:45,257 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,272 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,273 - __main__ - INFO - Generating token 4/16\n",
      "2024-12-15 20:18:45,274 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,290 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,291 - __main__ - INFO - Generating token 5/16\n",
      "2024-12-15 20:18:45,292 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,307 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,308 - __main__ - INFO - Generating token 6/16\n",
      "2024-12-15 20:18:45,310 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,327 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,327 - __main__ - INFO - Generating token 7/16\n",
      "2024-12-15 20:18:45,328 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,344 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,345 - __main__ - INFO - Generating token 8/16\n",
      "2024-12-15 20:18:45,346 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,361 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,362 - __main__ - INFO - Generating token 9/16\n",
      "2024-12-15 20:18:45,363 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,379 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,379 - __main__ - INFO - Generating token 10/16\n",
      "2024-12-15 20:18:45,380 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,396 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,396 - __main__ - INFO - Generating token 11/16\n",
      "2024-12-15 20:18:45,397 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,414 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,415 - __main__ - INFO - Generating token 12/16\n",
      "2024-12-15 20:18:45,416 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,431 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,432 - __main__ - INFO - Generating token 13/16\n",
      "2024-12-15 20:18:45,433 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,449 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,450 - __main__ - INFO - Generating token 14/16\n",
      "2024-12-15 20:18:45,451 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,466 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,467 - __main__ - INFO - Generating token 15/16\n",
      "2024-12-15 20:18:45,468 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,484 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,485 - __main__ - INFO - Generating token 16/16\n",
      "2024-12-15 20:18:45,486 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,502 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,517 - __main__ - INFO - Run 1/1\n",
      "2024-12-15 20:18:45,519 - __main__ - INFO - \n",
      "=== State during before prefill ===\n",
      "2024-12-15 20:18:45,558 - __main__ - INFO - \n",
      "=== State during after prefill ===\n",
      "2024-12-15 20:18:45,558 - __main__ - INFO - Generating token 1/16\n",
      "2024-12-15 20:18:45,560 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,641 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,642 - __main__ - INFO - Generating token 2/16\n",
      "2024-12-15 20:18:45,643 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,659 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,659 - __main__ - INFO - Generating token 3/16\n",
      "2024-12-15 20:18:45,661 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,676 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,677 - __main__ - INFO - Generating token 4/16\n",
      "2024-12-15 20:18:45,678 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,694 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,695 - __main__ - INFO - Generating token 5/16\n",
      "2024-12-15 20:18:45,696 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,711 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,712 - __main__ - INFO - Generating token 6/16\n",
      "2024-12-15 20:18:45,713 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,731 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,732 - __main__ - INFO - Generating token 7/16\n",
      "2024-12-15 20:18:45,733 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,750 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,750 - __main__ - INFO - Generating token 8/16\n",
      "2024-12-15 20:18:45,752 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,767 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,768 - __main__ - INFO - Generating token 9/16\n",
      "2024-12-15 20:18:45,769 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,785 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,785 - __main__ - INFO - Generating token 10/16\n",
      "2024-12-15 20:18:45,786 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,802 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,802 - __main__ - INFO - Generating token 11/16\n",
      "2024-12-15 20:18:45,803 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,819 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,820 - __main__ - INFO - Generating token 12/16\n",
      "2024-12-15 20:18:45,821 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,839 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,840 - __main__ - INFO - Generating token 13/16\n",
      "2024-12-15 20:18:45,841 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,857 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,858 - __main__ - INFO - Generating token 14/16\n",
      "2024-12-15 20:18:45,859 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,874 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,875 - __main__ - INFO - Generating token 15/16\n",
      "2024-12-15 20:18:45,876 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,892 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,892 - __main__ - INFO - Generating token 16/16\n",
      "2024-12-15 20:18:45,893 - __main__ - INFO - \n",
      "=== State during before decode step ===\n",
      "2024-12-15 20:18:45,909 - __main__ - INFO - \n",
      "=== State during after decode step ===\n",
      "2024-12-15 20:18:45,937 - __main__ - INFO - Generation time: 0.39s\n",
      "2024-12-15 20:18:45,938 - __main__ - INFO - Tokens per second: 40.91\n",
      "2024-12-15 20:18:45,940 - __main__ - INFO - CPU utilization: 1.1%\n"
     ]
    }
   ],
   "source": [
    "# Initial test configuration\n",
    "test_config = BenchmarkConfig(\n",
    "    context_lengths=[256],  # Start small\n",
    "    output_lengths=[16],    # Start small\n",
    "    num_runs=1,\n",
    "    warmup_runs=1\n",
    ")\n",
    "\n",
    "# Initialize and run benchmark\n",
    "benchmark = ModelBenchmark(test_config)\n",
    "results = benchmark.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e061038-c3e9-4959-bec2-2dea4ff1bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze and print benchmark results\"\"\"\n",
    "    print(\"\\n=== Benchmark Results ===\")\n",
    "    \n",
    "    # Group results by context length\n",
    "    for result in results:\n",
    "        print(f\"\\nContext Length: {result['context_length']}\")\n",
    "        print(f\"Output Length: {result['output_length']}\")\n",
    "        print(f\"Average Generation Time: {result['avg_time']:.2f}s ± {result['std_time']:.2f}s\")\n",
    "        print(f\"Tokens per Second: {result['tokens_per_second']:.2f}\")\n",
    "        print(\"\\nResource Usage:\")\n",
    "        print(f\"Peak GPU Memory: {result['gpu_memory_peak']:.0f}MB\")\n",
    "        print(f\"CPU Utilization:\")\n",
    "        print(f\"  - Average: {result['cpu_utilization']['mean']:.1f}%\")\n",
    "        print(f\"  - Peak: {result['cpu_utilization']['peak']:.1f}%\")\n",
    "        \n",
    "    # Save results to file\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    results_file = f\"benchmark_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nDetailed results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1410afe-5013-4dcf-ac35-9722b9533072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmark Results ===\n",
      "\n",
      "Context Length: 256\n",
      "Output Length: 16\n",
      "Average Generation Time: 0.39s ± 0.00s\n",
      "Tokens per Second: 40.91\n",
      "\n",
      "Resource Usage:\n",
      "Peak GPU Memory: 2448MB\n",
      "CPU Utilization:\n",
      "  - Average: 1.1%\n",
      "  - Peak: 100.0%\n",
      "\n",
      "Detailed results saved to: benchmark_results_20241215-201908.json\n"
     ]
    }
   ],
   "source": [
    "analyze_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b7f16-952c-4d5a-b8f3-1882807e55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_gpu = BenchmarkConfig(\n",
    "    model_name=\"facebook/opt-1.3b\",\n",
    "    context_lengths=[1024],\n",
    "    output_lengths=[50],\n",
    "    num_runs=2,\n",
    "    warmup_runs=1,\n",
    "    decode_strategy=\"gpu\"  # or \"gpu\"\n",
    ")\n",
    "\n",
    "benchmark_gpu = ModelBenchmark(config_gpu)\n",
    "results_gpu = benchmark.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f096a93-94e3-4643-ab59-86356a3d8181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmark Results ===\n",
      "\n",
      "Context Length: 1024\n",
      "Output Length: 50\n",
      "Average Generation Time: 14.15s ± 0.14s\n",
      "Tokens per Second: 3.53\n",
      "\n",
      "Resource Usage:\n",
      "Peak GPU Memory: 2798MB\n",
      "CPU Utilization:\n",
      "  - Average: 54.8%\n",
      "  - Peak: 100.0%\n",
      "\n",
      "Detailed results saved to: benchmark_results_20241113-182100.json\n"
     ]
    }
   ],
   "source": [
    "analyze_results(results_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7953b-dfa5-431f-88eb-04092ac03676",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    BenchmarkConfig(\n",
    "        context_lengths=[512], \n",
    "        output_lengths=[32],\n",
    "        decode_strategy=\"cpu\"\n",
    "    ),\n",
    "    BenchmarkConfig(\n",
    "        context_lengths=[512], \n",
    "        output_lengths=[32],\n",
    "        decode_strategy=\"gpu\"\n",
    "    ),\n",
    "    \n",
    "    BenchmarkConfig(\n",
    "        context_lengths=[2048], \n",
    "        output_lengths=[128],\n",
    "        decode_strategy=\"cpu\"\n",
    "    ),\n",
    "    BenchmarkConfig(\n",
    "        context_lengths=[2048], \n",
    "        output_lengths=[128],\n",
    "        decode_strategy=\"gpu\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Run all configurations\n",
    "all_results = []\n",
    "for config in configs:\n",
    "    benchmark = ModelBenchmark(config)\n",
    "    results = benchmark.run_benchmark()\n",
    "    all_results.append({\n",
    "        'config': asdict(config),\n",
    "        'results': results\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59e1d82-c61d-430b-b061-cc4e498cf421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Strategy Comparison ===\n",
      "\n",
      "Context Length: 512, Output Length: 32\n",
      "Speed Comparison (tokens/sec):\n",
      "  CPU: 5.07\n",
      "  GPU: 64.19\n",
      "  Relative Speed: 0.08x (>1 means CPU is faster)\n",
      "\n",
      "Memory Usage (MB):\n",
      "  CPU: 2666\n",
      "  GPU: 2770\n",
      "  Memory Savings: 3.8%\n",
      "\n",
      "CPU Utilization:\n",
      "  CPU decode: 38.6%\n",
      "  GPU decode: 35.3%\n",
      "\n",
      "Context Length: 2048, Output Length: 128\n",
      "Speed Comparison (tokens/sec):\n",
      "  CPU: 3.66\n",
      "  GPU: 65.15\n",
      "  Relative Speed: 0.06x (>1 means CPU is faster)\n",
      "\n",
      "Memory Usage (MB):\n",
      "  CPU: 2797\n",
      "  GPU: 2993\n",
      "  Memory Savings: 6.5%\n",
      "\n",
      "CPU Utilization:\n",
      "  CPU decode: 40.3%\n",
      "  GPU decode: 35.3%\n"
     ]
    }
   ],
   "source": [
    "def compare_strategies(all_results):\n",
    "    \"\"\"Compare CPU vs GPU decode strategies\"\"\"\n",
    "    print(\"\\n=== Strategy Comparison ===\")\n",
    "    \n",
    "    # Group by context and output lengths\n",
    "    grouped_results = {}\n",
    "    for run in all_results:\n",
    "        config = run['config']\n",
    "        results = run['results'][0]  # Take first result for each config\n",
    "        \n",
    "        key = (config['context_lengths'][0], config['output_lengths'][0])\n",
    "        if key not in grouped_results:\n",
    "            grouped_results[key] = {}\n",
    "            \n",
    "        grouped_results[key][config['decode_strategy']] = {\n",
    "            'tokens_per_second': results['tokens_per_second'],\n",
    "            'gpu_memory': results['gpu_memory_peak'],\n",
    "            'cpu_util': results['cpu_utilization']['mean']\n",
    "        }\n",
    "    \n",
    "    # Print comparisons\n",
    "    for (ctx_len, out_len), strategies in grouped_results.items():\n",
    "        print(f\"\\nContext Length: {ctx_len}, Output Length: {out_len}\")\n",
    "        cpu_stats = strategies.get('cpu', {})\n",
    "        gpu_stats = strategies.get('gpu', {})\n",
    "        \n",
    "        if cpu_stats and gpu_stats:\n",
    "            speedup = cpu_stats['tokens_per_second'] / gpu_stats['tokens_per_second']\n",
    "            memory_saving = 1 - (cpu_stats['gpu_memory'] / gpu_stats['gpu_memory'])\n",
    "            \n",
    "            print(f\"Speed Comparison (tokens/sec):\")\n",
    "            print(f\"  CPU: {cpu_stats['tokens_per_second']:.2f}\")\n",
    "            print(f\"  GPU: {gpu_stats['tokens_per_second']:.2f}\")\n",
    "            print(f\"  Relative Speed: {speedup:.2f}x (>1 means CPU is faster)\")\n",
    "            \n",
    "            print(f\"\\nMemory Usage (MB):\")\n",
    "            print(f\"  CPU: {cpu_stats['gpu_memory']:.0f}\")\n",
    "            print(f\"  GPU: {gpu_stats['gpu_memory']:.0f}\")\n",
    "            print(f\"  Memory Savings: {memory_saving*100:.1f}%\")\n",
    "            \n",
    "            print(f\"\\nCPU Utilization:\")\n",
    "            print(f\"  CPU decode: {cpu_stats['cpu_util']:.1f}%\")\n",
    "            print(f\"  GPU decode: {gpu_stats['cpu_util']:.1f}%\")\n",
    "\n",
    "compare_strategies(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053eeb9-bd96-4258-800e-ac9c18e4094b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
