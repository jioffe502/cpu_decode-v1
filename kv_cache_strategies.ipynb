{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cae794c-35bc-458e-80c1-9a76c5e45745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load HF token from .env\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3955969b-a0ad-447b-b80e-8bd45755ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, List, Union, Tuple, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    PretrainedConfig,\n",
    "    Cache,\n",
    "    DynamicCache, \n",
    "    OffloadedCache,\n",
    "    QuantizedCache,\n",
    "    QuantizedCacheConfig,\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef92b4-79f0-4290-86ed-bd56eb8e2cba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57bc4e39-9872-4f1c-a2e4-1bd5a7cac290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDocumentLoader:\n",
    "    def __init__(self, file_path: str = \"data/crimeandpunishment.txt\"):\n",
    "        self.file_path = Path(file_path)\n",
    "        self.text_cache = None\n",
    "        \n",
    "    def load_chunk(self, start_pos: int = 1000, chunk_size: int = 1000) -> str:\n",
    "        \"\"\"Load a chunk from the document with caching\"\"\"\n",
    "        # Cache the full text on first load\n",
    "        if self.text_cache is None:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Skip Project Gutenberg header\n",
    "            start_marker = \"CRIME AND PUNISHMENT\"\n",
    "            narrative_start = text.find(start_marker)\n",
    "            if narrative_start == -1:\n",
    "                raise ValueError(f\"Start marker '{start_marker}' not found in the document.\")\n",
    "            self.text_cache = text[narrative_start:]\n",
    "        \n",
    "        # Get clean chunk\n",
    "        chunk_start = min(start_pos, len(self.text_cache) - chunk_size)\n",
    "        chunk = self.text_cache[chunk_start:chunk_start + chunk_size]\n",
    "        \n",
    "        # Adjust to sentence boundary\n",
    "        first_period = chunk.find(\". \") + 2\n",
    "        if first_period > 1:\n",
    "            chunk = chunk[first_period:]\n",
    "            \n",
    "        return chunk\n",
    "\n",
    "    def get_total_length(self) -> int:\n",
    "        \"\"\"Get total length of usable text\"\"\"\n",
    "        if self.text_cache is None:\n",
    "            _ = self.load_chunk()  # This will load and cache the text\n",
    "        return len(self.text_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e4db9-7de0-47af-b92c-967373f159db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649754c6-e4de-4fcc-9282-5d588c7a1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQuantizedCache(QuantizedCache):\n",
    "    \"\"\"Basic implementation of quantized cache using simple min-max quantization\"\"\"\n",
    "    \n",
    "    def _quantize(self, tensor: torch.Tensor, axis: int) -> torch.Tensor:\n",
    "        # Simple min-max quantization\n",
    "        with torch.no_grad():\n",
    "            # Compute min and max along the specified axis\n",
    "            if axis == 0:\n",
    "                min_val = tensor.min(dim=0)[0]\n",
    "                max_val = tensor.max(dim=0)[0]\n",
    "            else:  # axis = -1\n",
    "                min_val = tensor.min(dim=-1, keepdim=True)[0]\n",
    "                max_val = tensor.max(dim=-1, keepdim=True)[0]\n",
    "            \n",
    "            # Scale to [0, 2^nbits - 1]\n",
    "            scale = (max_val - min_val) / (2**self.nbits - 1)\n",
    "            scale = torch.clamp(scale, min=1e-6)  # Prevent division by zero\n",
    "            \n",
    "            # Quantize\n",
    "            qtensor = ((tensor - min_val) / scale).round().clamp(0, 2**self.nbits - 1)\n",
    "            \n",
    "            # Store scaling factors as attributes of the tensor\n",
    "            qtensor.scale = scale\n",
    "            qtensor.zero_point = min_val\n",
    "            \n",
    "            return qtensor\n",
    "    \n",
    "    def _dequantize(self, qtensor: torch.Tensor) -> torch.Tensor:\n",
    "        # Dequantize using stored scale and zero point\n",
    "        with torch.no_grad():\n",
    "            return qtensor * qtensor.scale + qtensor.zero_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7e883-081d-46a1-b5e0-4f8f2c645844",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e79f4a5-25f3-4ad2-bc25-1558485ad679",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CPUDecodeConfig:\n",
    "    \"\"\"Configuration for CPU decode settings\"\"\"\n",
    "    enabled: bool = False\n",
    "    # Which layers to place on CPU (None means auto-select)\n",
    "    cpu_layers: Optional[List[int]] = None\n",
    "    # Whether to use mixed precision on CPU\n",
    "    use_fp32_cpu: bool = True\n",
    "\n",
    "@dataclass\n",
    "class CacheConfig:\n",
    "    \"\"\"Configuration for cache strategies with CPU decode support\"\"\"\n",
    "    strategy: str\n",
    "    decode_on_cpu: bool = False\n",
    "    quantization: Optional[Dict] = None\n",
    "    cpu_decode: Optional[CPUDecodeConfig] = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cache(config: \"CacheConfig\", model_config) -> Optional[Cache]:\n",
    "        \"\"\"Initialize appropriate cache based on configuration\"\"\"\n",
    "        if config.strategy == \"dynamic\":\n",
    "            return DynamicCache()\n",
    "        elif config.strategy == \"quantized\":\n",
    "            if config.quantization is None:\n",
    "                config.quantization = {\n",
    "                    'nbits': 4,\n",
    "                    'residual_length': 128,\n",
    "                    'compute_dtype': torch.float16\n",
    "                }\n",
    "            quant_config = QuantizedCacheConfig(**config.quantization)\n",
    "            return SimpleQuantizedCache(cache_config=quant_config)\n",
    "        elif config.strategy == \"offloaded\":\n",
    "            return OffloadedCache()\n",
    "        return None\n",
    "\n",
    "    def get_layer_mapping(self, num_layers: int) -> Dict[str, str]:\n",
    "        \"\"\"Get device mapping for model layers based on config\"\"\"\n",
    "        if not self.cpu_decode or not self.cpu_decode.enabled:\n",
    "            return None\n",
    "            \n",
    "        mapping = {\n",
    "            # Keep embedding and other components on GPU\n",
    "            \"model.embed_tokens\": \"cuda:0\",\n",
    "            \"model.norm\": \"cuda:0\",\n",
    "            \"lm_head\": \"cuda:0\",\n",
    "            \"model.layers\": \"cuda:0\"  # Default for any unspecified layers\n",
    "        }\n",
    "        \n",
    "        if self.cpu_decode.cpu_layers is None:\n",
    "            # Default strategy: Put latter half of layers on CPU\n",
    "            split_point = num_layers // 2\n",
    "            cpu_layers = list(range(split_point, num_layers))\n",
    "        else:\n",
    "            cpu_layers = self.cpu_decode.cpu_layers\n",
    "            \n",
    "        for i in range(num_layers):\n",
    "            if i in cpu_layers:\n",
    "                mapping[f\"model.layers.{i}\"] = \"cpu\"\n",
    "            else:\n",
    "                mapping[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "                \n",
    "        return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66caec-1be6-479e-b279-c8b7efaf5ff7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f3e603-166c-4ac1-9d1c-54ecb101df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferencePhaseMetrics:\n",
    "    \"\"\"Detailed metrics for each inference phase (prefill/decode)\"\"\"\n",
    "    phase: str  # 'prefill' or 'decode'\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    tokens_processed: int\n",
    "    gpu_memory: Dict[str, float]  # Detailed memory stats\n",
    "    cpu_memory: Dict[str, float]\n",
    "    gpu_util: float\n",
    "    memory_bandwidth: float\n",
    "    cache_metrics: Dict[str, float]  # Cache-specific metrics\n",
    "\n",
    "@dataclass\n",
    "class TokenGenerationMetrics:\n",
    "    \"\"\"Enhanced per-token metrics\"\"\"\n",
    "    timestamp: float\n",
    "    token_index: int\n",
    "    phase: str\n",
    "    latency: float\n",
    "    gpu_memory: Dict[str, float]\n",
    "    cpu_memory: Dict[str, float]\n",
    "    gpu_util: float\n",
    "    cache_size: int\n",
    "    cache_metrics: Optional[Dict[str, float]] = None\n",
    "\n",
    "class EnhancedMetricsTracker:\n",
    "    \"\"\"Enhanced metrics tracking with phase awareness and detailed memory stats\"\"\"\n",
    "    def __init__(self, sampling_rate: float = 0.1):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.start_time = time.perf_counter()\n",
    "        self.current_phase = None\n",
    "        self.phase_metrics: List[InferencePhaseMetrics] = []\n",
    "        self.token_metrics: List[TokenGenerationMetrics] = []\n",
    "        \n",
    "        # Initialize CUDA events for memory bandwidth tracking\n",
    "        if torch.cuda.is_available():\n",
    "            self.start_event = torch.cuda.Event(enable_timing=True)\n",
    "            self.end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    def start_phase(self, phase: str):\n",
    "        \"\"\"Start tracking a new inference phase\"\"\"\n",
    "        self.current_phase = phase\n",
    "        self.start_event.record()\n",
    "        \n",
    "        self.phase_metrics.append(InferencePhaseMetrics(\n",
    "            phase=phase,\n",
    "            start_time=time.perf_counter(),\n",
    "            end_time=0,\n",
    "            tokens_processed=0,\n",
    "            gpu_memory=self._get_gpu_memory_stats(),\n",
    "            cpu_memory=self._get_cpu_memory_stats(),\n",
    "            gpu_util=self._get_gpu_utilization(),\n",
    "            memory_bandwidth=0,\n",
    "            cache_metrics={}\n",
    "        ))\n",
    "    \n",
    "    def end_phase(self, tokens_processed: int, cache_metrics: Optional[Dict[str, float]] = None):\n",
    "        \"\"\"End current phase and record final metrics\"\"\"\n",
    "        self.end_event.record()\n",
    "        self.end_event.synchronize()\n",
    "        \n",
    "        current_metrics = self.phase_metrics[-1]\n",
    "        current_metrics.end_time = time.perf_counter()\n",
    "        current_metrics.tokens_processed = tokens_processed\n",
    "        current_metrics.memory_bandwidth = self._calculate_memory_bandwidth()\n",
    "        if cache_metrics:\n",
    "            current_metrics.cache_metrics.update(cache_metrics)\n",
    "    \n",
    "    def sample_token(self, token_index: int, phase: str, latency: float, cache_size: int,\n",
    "                    cache_metrics: Optional[Dict[str, float]] = None):\n",
    "        \"\"\"Record metrics for a single token generation\"\"\"\n",
    "        self.token_metrics.append(TokenGenerationMetrics(\n",
    "            timestamp=time.perf_counter() - self.start_time,\n",
    "            token_index=token_index,\n",
    "            phase=phase,\n",
    "            latency=latency,\n",
    "            gpu_memory=self._get_gpu_memory_stats(),\n",
    "            cpu_memory=self._get_cpu_memory_stats(),\n",
    "            gpu_util=self._get_gpu_utilization(),\n",
    "            cache_size=cache_size,\n",
    "            cache_metrics=cache_metrics\n",
    "        ))\n",
    "    \n",
    "    def _get_gpu_memory_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get detailed GPU memory statistics\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {}\n",
    "            \n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / 1024**2,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1024**2,\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / 1024**2,\n",
    "            'max_reserved': torch.cuda.max_memory_reserved() / 1024**2,\n",
    "            'fragmentation': self._calculate_memory_fragmentation()\n",
    "        }\n",
    "    \n",
    "    def _get_cpu_memory_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get detailed CPU memory statistics\"\"\"\n",
    "        process = psutil.Process()\n",
    "        return {\n",
    "            'rss': process.memory_info().rss / 1024**2,\n",
    "            'vms': process.memory_info().vms / 1024**2,\n",
    "            'shared': process.memory_info().shared / 1024**2,\n",
    "            'percent': process.memory_percent()\n",
    "        }\n",
    "    \n",
    "    def _calculate_memory_fragmentation(self) -> float:\n",
    "        \"\"\"Calculate memory fragmentation with improved accuracy\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0.0\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        reserved = torch.cuda.memory_reserved()\n",
    "        \n",
    "        if reserved == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Calculate fragmentation in MB\n",
    "        fragmentation = (reserved - allocated) / 1024**2\n",
    "        \n",
    "        return fragmentation\n",
    "    \n",
    "    def _get_gpu_utilization(self) -> float:\n",
    "        \"\"\"Get GPU utilization percentage with improved accuracy\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0.0\n",
    "        try:\n",
    "            # Try using nvidia-smi through subprocess if available\n",
    "            import subprocess\n",
    "            result = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            return float(result.strip()) / 100.0\n",
    "        except:\n",
    "            # Fallback to torch CUDA metrics\n",
    "            return torch.cuda.utilization() / 100.0\n",
    "    \n",
    "    def _calculate_memory_bandwidth(self) -> float:\n",
    "        \"\"\"Calculate memory bandwidth usage between events\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0.0\n",
    "        return self.start_event.elapsed_time(self.end_event) * 1e-3  # Convert to seconds\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get enhanced statistical summary of collected metrics\"\"\"\n",
    "        # Debug print\n",
    "        logger.debug(f\"Number of token metrics collected: {len(self.token_metrics)}\")\n",
    "        logger.debug(f\"Phases recorded: {set(m.phase for m in self.token_metrics)}\")\n",
    "        \n",
    "        summary = {\n",
    "            'phases': {},\n",
    "            'memory': {\n",
    "                'peak_gpu_allocated': max(m.gpu_memory.get('allocated', 0) for m in self.token_metrics),\n",
    "                'peak_gpu_reserved': max(m.gpu_memory.get('reserved', 0) for m in self.token_metrics),\n",
    "                'peak_cpu': max(m.cpu_memory.get('rss', 0) for m in self.token_metrics),\n",
    "                'mean_fragmentation': np.mean([m.gpu_memory.get('fragmentation', 0) for m in self.token_metrics]),\n",
    "            },\n",
    "            'performance': {\n",
    "                'mean_latency': np.mean([m.latency for m in self.token_metrics]),\n",
    "                'p90_latency': np.percentile([m.latency for m in self.token_metrics], 90),\n",
    "                'mean_gpu_util': np.mean([m.gpu_util for m in self.token_metrics]),\n",
    "                'tokens_per_second': len(self.token_metrics) / (self.token_metrics[-1].timestamp if self.token_metrics else 1)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add phase-specific metrics\n",
    "        for phase in ['prefill', 'decode']:\n",
    "            phase_tokens = [m for m in self.token_metrics if m.phase == phase]\n",
    "            if phase_tokens:\n",
    "                summary['phases'][phase] = {\n",
    "                    'token_count': len(phase_tokens),\n",
    "                    'mean_latency': np.mean([m.latency for m in phase_tokens]),\n",
    "                    'peak_memory': max(m.gpu_memory.get('allocated', 0) for m in phase_tokens),\n",
    "                    'mean_cache_size': np.mean([m.cache_size for m in phase_tokens])\n",
    "                }\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ae901-b72f-4649-98d7-83bf27b64b7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "695ff354-6354-4835-9191-678f141fa0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDeviceManager:\n",
    "    \"\"\"Manages device placement for Llama model without modifying its internals\"\"\"\n",
    "    \n",
    "    def __init__(self, model, cpu_layers):\n",
    "        self.model = model\n",
    "        self.cpu_layers = set(cpu_layers)\n",
    "        self.hooks = []\n",
    "        self.setup_hooks()\n",
    "        \n",
    "    def setup_hooks(self):\n",
    "        \"\"\"Setup forward pre-hooks for device management\"\"\"\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'layers' in name:\n",
    "                layer_idx = int(name.split('.')[-1]) if name.split('.')[-1].isdigit() else -1\n",
    "                if layer_idx in self.cpu_layers:\n",
    "                    hook = module.register_forward_pre_hook(\n",
    "                        lambda mod, inp, layer_idx=layer_idx: self._layer_pre_hook(mod, inp, layer_idx)\n",
    "                    )\n",
    "                    self.hooks.append(hook)\n",
    "    \n",
    "    def _layer_pre_hook(self, module, inputs, layer_idx):\n",
    "        \"\"\"Handle device transitions before layer execution\"\"\"\n",
    "        if not isinstance(inputs, tuple):\n",
    "            inputs = (inputs,)\n",
    "            \n",
    "        # Move inputs to appropriate device\n",
    "        device = torch.device('cpu') if layer_idx in self.cpu_layers else torch.device('cuda:0')\n",
    "        processed_inputs = []\n",
    "        \n",
    "        for x in inputs:\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                processed_inputs.append(x.to(device))\n",
    "            elif isinstance(x, tuple):\n",
    "                processed_inputs.append(tuple(t.to(device) if isinstance(t, torch.Tensor) else t for t in x))\n",
    "            else:\n",
    "                processed_inputs.append(x)\n",
    "                \n",
    "        return tuple(processed_inputs)\n",
    "\n",
    "    def prepare_inputs(self, input_ids, attention_mask=None, position_ids=None):\n",
    "        \"\"\"Prepare inputs with proper device placement\"\"\"\n",
    "        device = torch.device('cuda:0')  # Start on GPU\n",
    "        prepared_inputs = {\n",
    "            'input_ids': input_ids.to(device),\n",
    "            'attention_mask': attention_mask.to(device) if attention_mask is not None else None,\n",
    "            'position_ids': position_ids.to(device) if position_ids is not None else None\n",
    "        }\n",
    "        return prepared_inputs\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove hooks when done\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "\n",
    "class EnhancedLlamaInference:\n",
    "    \"\"\"Enhanced inference wrapper with fixed device management\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, cache_config: Optional[CacheConfig] = None):\n",
    "        self.cache_config = cache_config or CacheConfig(strategy=\"dynamic\")\n",
    "        \n",
    "        # Create device map\n",
    "        device_map = {\"model.embed_tokens\": \"cpu\"}  # Keep embeddings on CPU\n",
    "        if self.cache_config.cpu_decode and self.cache_config.cpu_decode.enabled:\n",
    "            for i in range(16):  # Assuming 16 layers\n",
    "                if i in self.cache_config.cpu_decode.cpu_layers:\n",
    "                    device_map[f\"model.layers.{i}\"] = \"cpu\"\n",
    "                else:\n",
    "                    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "        else:\n",
    "            device_map = \"auto\"\n",
    "            \n",
    "        # Load model with device map\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device_map,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        # Initialize cache\n",
    "        self.cache = CacheConfig.get_cache(self.cache_config, self.model.config)\n",
    "        \n",
    "    def run_inference(\n",
    "        self,\n",
    "        input_text: str,\n",
    "        max_new_tokens: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "    ) -> Dict:\n",
    "        metrics_tracker = EnhancedMetricsTracker()\n",
    "        \n",
    "        try:\n",
    "            # Tokenize on CPU first\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Keep inputs on CPU initially since embed_tokens is on CPU\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs.get(\"attention_mask\")\n",
    "            \n",
    "            metrics_tracker.start_phase('prefill')\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,  # Keep on CPU\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    past_key_values=self.cache,\n",
    "                    use_cache=True,\n",
    "                    # Ensure proper device handling for cache\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "                \n",
    "            metrics_tracker.end_phase(\n",
    "                tokens_processed=outputs.shape[1],\n",
    "                cache_metrics=self._get_cache_metrics()\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"text\": self.tokenizer.decode(outputs[0], skip_special_tokens=True),\n",
    "                \"metrics\": metrics_tracker.get_summary()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _get_cache_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Get cache memory metrics\"\"\"\n",
    "        if not hasattr(self.cache, 'get_seq_length'):\n",
    "            return {}\n",
    "            \n",
    "        return {\n",
    "            'cache_size': self.cache.get_seq_length(),\n",
    "            'gpu_memory': torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0,\n",
    "        }\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        del self.model\n",
    "        del self.cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984998e-1753-400f-b4ca-8a672ba692d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b99503-555a-4b6c-9fe5-8871282538fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model 'meta-llama/Llama-3.2-1B' with cache strategy 'dynamic'\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced benchmark...\n",
      "\n",
      "Testing maximum context lengths:\n",
      "\n",
      "Strategy: dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model 'meta-llama/Llama-3.2-1B' with cache strategy 'dynamic'\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 170\u001b[0m\n\u001b[1;32m    167\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting enhanced benchmark...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_enhanced_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Print results in a structured format\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBenchmark Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 114\u001b[0m, in \u001b[0;36mrun_enhanced_benchmark\u001b[0;34m(strategies, context_checkpoints)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m strategy \u001b[38;5;129;01min\u001b[39;00m strategies:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     max_length, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mfind_max_context_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_context\u001b[39m\u001b[38;5;124m'\u001b[39m][strategy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m: max_length,\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics\n\u001b[1;32m    118\u001b[0m     }\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax context length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m, in \u001b[0;36mfind_max_context_length\u001b[0;34m(strategy, start_length, max_length, tolerance, model_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m input_text \u001b[38;5;241m=\u001b[39m document_loader\u001b[38;5;241m.\u001b[39mload_chunk(chunk_size\u001b[38;5;241m=\u001b[39mtest_length)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Run inference with small output to test context handling\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Small output for testing\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Get memory metrics\u001b[39;00m\n\u001b[1;32m     43\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m'\u001b[39m: test_length,\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_memory\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m: strategy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     49\u001b[0m }\n",
      "Cell \u001b[0;32mIn[7], line 132\u001b[0m, in \u001b[0;36mLlamaInference.run_inference\u001b[0;34m(self, input_text, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    125\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    126\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    127\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache,\n\u001b[1;32m    128\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m prefill_latency \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mperf_counter_ns() \u001b[38;5;241m-\u001b[39m prefill_start) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e9\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Record prefill metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/proteomics/lib/python3.11/site-packages/torch/cuda/__init__.py:688\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    686\u001b[0m _lazy_init()\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def find_max_context_length(\n",
    "    strategy: Dict,\n",
    "    start_length: int = 1024,\n",
    "    max_length: int = 32768,\n",
    "    tolerance: int = 512,\n",
    "    model_name: str = \"meta-llama/Llama-3.2-1B\"\n",
    ") -> Tuple[int, Dict]:\n",
    "    \"\"\"\n",
    "    Binary search to find maximum context length for a given strategy.\n",
    "    Returns (max_length, metrics)\n",
    "    \"\"\"\n",
    "    left, right = start_length, max_length\n",
    "    max_successful_length = 0\n",
    "    max_successful_metrics = None\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        # Round to nearest multiple of tolerance\n",
    "        test_length = (mid // tolerance) * tolerance\n",
    "        \n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Initialize model with strategy\n",
    "            llm = LlamaInference(\n",
    "                model_name=model_name,\n",
    "                cache_config=strategy['config']\n",
    "            )\n",
    "            \n",
    "            # Load test data\n",
    "            document_loader = EnhancedDocumentLoader()\n",
    "            input_text = document_loader.load_chunk(chunk_size=test_length)\n",
    "            \n",
    "            # Run inference with small output to test context handling\n",
    "            result = llm.run_inference(\n",
    "                input_text=input_text,\n",
    "                max_new_tokens=20  # Small output for testing\n",
    "            )\n",
    "            \n",
    "            # Get memory metrics\n",
    "            metrics = {\n",
    "                'context_length': test_length,\n",
    "                'peak_memory': torch.cuda.max_memory_allocated()/1024**2,\n",
    "                'reserved_memory': torch.cuda.max_memory_reserved()/1024**2,\n",
    "                'throughput': result['metrics']['performance']['tokens_per_second'],\n",
    "                'strategy': strategy['name']\n",
    "            }\n",
    "            \n",
    "            # Update maximum successful length\n",
    "            max_successful_length = test_length\n",
    "            max_successful_metrics = metrics\n",
    "            \n",
    "            # Try larger context\n",
    "            left = mid + 1\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # Try smaller context\n",
    "            right = mid - 1\n",
    "        \n",
    "        finally:\n",
    "            if 'llm' in locals():\n",
    "                llm.cleanup()\n",
    "    \n",
    "    return max_successful_length, max_successful_metrics\n",
    "\n",
    "def run_enhanced_benchmark(\n",
    "    strategies: List[Dict] = None,\n",
    "    context_checkpoints: List[int] = [4096, 8192, 16384, 32768],\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced benchmark that:\n",
    "    1. Finds max context length for each strategy\n",
    "    2. Tests performance at various context lengths up to max\n",
    "    \"\"\"\n",
    "    if strategies is None:\n",
    "        strategies = [\n",
    "            {\n",
    "                \"name\": \"dynamic\",\n",
    "                \"config\": CacheConfig(strategy=\"dynamic\", decode_on_cpu=False)\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"quantized_4bit\",\n",
    "                \"config\": CacheConfig(\n",
    "                    strategy=\"quantized\",\n",
    "                    decode_on_cpu=False,\n",
    "                    quantization={'nbits': 4, 'residual_length': 128}\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"quantized_2bit\",\n",
    "                \"config\": CacheConfig(\n",
    "                    strategy=\"quantized\",\n",
    "                    decode_on_cpu=False,\n",
    "                    quantization={'nbits': 2, 'residual_length': 128}\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"offloaded\",\n",
    "                \"config\": CacheConfig(strategy=\"offloaded\", decode_on_cpu=False)\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    results = {\n",
    "        'max_context': {},\n",
    "        'performance_curve': {},\n",
    "    }\n",
    "    \n",
    "    # Find max context length for each strategy\n",
    "    print(\"\\nTesting maximum context lengths:\")\n",
    "    for strategy in strategies:\n",
    "        print(f\"\\nStrategy: {strategy['name']}\")\n",
    "        max_length, metrics = find_max_context_length(strategy)\n",
    "        results['max_context'][strategy['name']] = {\n",
    "            'max_length': max_length,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        print(f\"Max context length: {max_length}\")\n",
    "        print(f\"Peak memory: {metrics['peak_memory']:.0f}MB\")\n",
    "    \n",
    "    # Test performance at different context lengths\n",
    "    print(\"\\nTesting performance curve:\")\n",
    "    for strategy in strategies:\n",
    "        max_length = results['max_context'][strategy['name']]['max_length']\n",
    "        results['performance_curve'][strategy['name']] = []\n",
    "        \n",
    "        # Test at checkpoints up to max length\n",
    "        valid_checkpoints = [l for l in context_checkpoints if l <= max_length]\n",
    "        \n",
    "        for length in valid_checkpoints:\n",
    "            try:\n",
    "                llm = LlamaInference(\n",
    "                    model_name=\"meta-llama/Llama-3.2-1B\", # change llama\n",
    "                    cache_config=strategy['config']\n",
    "                )\n",
    "                \n",
    "                document_loader = EnhancedDocumentLoader()\n",
    "                input_text = document_loader.load_chunk(chunk_size=length)\n",
    "                \n",
    "                result = llm.run_inference(\n",
    "                    input_text=input_text,\n",
    "                    max_new_tokens=20\n",
    "                )\n",
    "                \n",
    "                metrics = {\n",
    "                    'context_length': length,\n",
    "                    'peak_memory': torch.cuda.max_memory_allocated()/1024**2,\n",
    "                    'reserved_memory': torch.cuda.max_memory_reserved()/1024**2,\n",
    "                    'throughput': result['metrics']['performance']['tokens_per_second'],\n",
    "                    'latency': result['metrics']['performance']['mean_latency']\n",
    "                }\n",
    "                \n",
    "                results['performance_curve'][strategy['name']].append(metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error testing {strategy['name']} at length {length}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            finally:\n",
    "                if 'llm' in locals():\n",
    "                    llm.cleanup()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    print(\"Starting enhanced benchmark...\")\n",
    "    \n",
    "    results = run_enhanced_benchmark()\n",
    "    \n",
    "    # Print results in a structured format\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nMaximum Context Lengths:\")\n",
    "    for strategy, data in results['max_context'].items():\n",
    "        print(f\"\\nStrategy: {strategy}\")\n",
    "        print(f\"Max Length: {data['max_length']}\")\n",
    "        print(f\"Peak Memory: {data['metrics']['peak_memory']:.0f}MB\")\n",
    "        print(f\"Throughput: {data['metrics']['throughput']:.2f} tokens/s\")\n",
    "    \n",
    "    print(\"\\nPerformance Curves:\")\n",
    "    for strategy, curve in results['performance_curve'].items():\n",
    "        print(f\"\\nStrategy: {strategy}\")\n",
    "        for point in curve:\n",
    "            print(f\"Context {point['context_length']}: \"\n",
    "                  f\"{point['throughput']:.2f} tokens/s, \"\n",
    "                  f\"{point['peak_memory']:.0f}MB peak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8fcb5-8726-4505-8d99-24e0a6eafd75",
   "metadata": {},
   "source": [
    "# CPU Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff07d78-8f08-4f86-82c5-2cba105538fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PretrainedConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mCPUDecodeManager\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Manages CPU decode offloading for Llama model layers\"\"\"\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPretrainedConfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_layers\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_fp32\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mCPUDecodeManager\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCPUDecodeManager\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manages CPU decode offloading for Llama model layers\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m---> 10\u001b[0m         model_config: \u001b[43mPretrainedConfig\u001b[49m,\n\u001b[1;32m     11\u001b[0m         cpu_layers: Optional[List[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m         use_fp32: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     ):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_layers \u001b[38;5;241m=\u001b[39m cpu_layers \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PretrainedConfig' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional, List\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class CPUDecodeManager:\n",
    "    \"\"\"Manages CPU decode offloading for Llama model layers\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: PretrainedConfig,\n",
    "        cpu_layers: Optional[List[int]] = None,\n",
    "        use_fp32: bool = True,\n",
    "    ):\n",
    "        self.num_layers = model_config.num_hidden_layers\n",
    "        self.cpu_layers = cpu_layers or list(range(self.num_layers))\n",
    "        self.use_fp32 = use_fp32\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Track device mapping\n",
    "        self.layer_devices: Dict[int, str] = {}\n",
    "        self._initialize_device_mapping()\n",
    "        \n",
    "    def _initialize_device_mapping(self):\n",
    "        \"\"\"Initialize the device mapping for all layers\"\"\"\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            if layer_idx in self.cpu_layers:\n",
    "                self.layer_devices[layer_idx] = \"cpu\"\n",
    "            else:\n",
    "                self.layer_devices[layer_idx] = \"cuda:0\"\n",
    "                \n",
    "    def prepare_inputs_for_layer(\n",
    "        self,\n",
    "        layer_idx: int,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Move inputs to appropriate device for layer\"\"\"\n",
    "        target_device = self.layer_devices[layer_idx]\n",
    "        \n",
    "        # Convert to fp32 if on CPU and configured\n",
    "        if target_device == \"cpu\" and self.use_fp32:\n",
    "            hidden_states = hidden_states.to(dtype=torch.float32)\n",
    "            \n",
    "        # Move tensors to target device\n",
    "        hidden_states = hidden_states.to(target_device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(target_device)\n",
    "            \n",
    "        return hidden_states, attention_mask\n",
    "        \n",
    "    def prepare_layer(self, layer: nn.Module, layer_idx: int):\n",
    "        \"\"\"Move layer to appropriate device and dtype\"\"\"\n",
    "        target_device = self.layer_devices[layer_idx]\n",
    "        target_dtype = torch.float32 if (target_device == \"cpu\" and self.use_fp32) else torch.float16\n",
    "        \n",
    "        # Move layer parameters\n",
    "        layer.to(device=target_device, dtype=target_dtype)\n",
    "        \n",
    "    def get_layer_device(self, layer_idx: int) -> str:\n",
    "        \"\"\"Get target device for given layer\"\"\"\n",
    "        return self.layer_devices.get(layer_idx, \"cuda:0\")\n",
    "        \n",
    "    def optimize_memory_transfers(self, hidden_states: torch.Tensor, from_layer: int, to_layer: int) -> torch.Tensor:\n",
    "        \"\"\"Optimize memory transfers between layers\"\"\"\n",
    "        from_device = self.get_layer_device(from_layer)\n",
    "        to_device = self.get_layer_device(to_layer)\n",
    "        \n",
    "        if from_device != to_device:\n",
    "            # Handle dtype conversion if needed\n",
    "            if to_device == \"cpu\" and self.use_fp32:\n",
    "                hidden_states = hidden_states.to(dtype=torch.float32)\n",
    "            elif from_device == \"cpu\" and self.use_fp32:\n",
    "                hidden_states = hidden_states.to(dtype=torch.float16)\n",
    "                \n",
    "            hidden_states = hidden_states.to(to_device)\n",
    "            \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea861bdb-230c-491b-8b5d-82886c16e967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
