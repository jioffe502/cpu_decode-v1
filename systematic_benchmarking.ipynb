{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af670b5-05c8-4a8c-9ba4-8916eaaa7c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 16:35:29 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    62W / 250W |    837MiB / 22698MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    118101      C   ...nvs/proteomics/bin/python      834MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae937ae-c719-42b7-ac9d-5f1572a321af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load HF token from .env\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf011ed-4dda-4e18-bc8c-d6d1eeb2136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import psutil\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, List, Union, Tuple, Any\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    Cache,\n",
    "    DynamicCache, \n",
    "    OffloadedCache,\n",
    "    QuantizedCache,\n",
    "    QuantizedCacheConfig\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedDocumentLoader:\n",
    "    def __init__(self, file_path: str = \"data/crimeandpunishment.txt\"):\n",
    "        self.file_path = Path(file_path)\n",
    "        self.text_cache = None\n",
    "        \n",
    "    def load_chunk(self, start_pos: int = 1000, chunk_size: int = 1000) -> str:\n",
    "        \"\"\"Load a chunk from the document with caching\"\"\"\n",
    "        # Cache the full text on first load\n",
    "        if self.text_cache is None:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Skip Project Gutenberg header\n",
    "            start_marker = \"CRIME AND PUNISHMENT\"\n",
    "            narrative_start = text.find(start_marker)\n",
    "            if narrative_start == -1:\n",
    "                raise ValueError(f\"Start marker '{start_marker}' not found in the document.\")\n",
    "            self.text_cache = text[narrative_start:]\n",
    "        \n",
    "        # Get clean chunk\n",
    "        chunk_start = min(start_pos, len(self.text_cache) - chunk_size)\n",
    "        chunk = self.text_cache[chunk_start:chunk_start + chunk_size]\n",
    "        \n",
    "        # Adjust to sentence boundary\n",
    "        first_period = chunk.find(\". \") + 2\n",
    "        if first_period > 1:\n",
    "            chunk = chunk[first_period:]\n",
    "            \n",
    "        return chunk\n",
    "\n",
    "    def get_total_length(self) -> int:\n",
    "        \"\"\"Get total length of usable text\"\"\"\n",
    "        if self.text_cache is None:\n",
    "            _ = self.load_chunk()  # This will load and cache the text\n",
    "        return len(self.text_cache)\n",
    "\n",
    "\n",
    "class SimpleQuantizedCache(QuantizedCache):\n",
    "    \"\"\"Basic implementation of quantized cache using simple min-max quantization\"\"\"\n",
    "    \n",
    "    def _quantize(self, tensor: torch.Tensor, axis: int) -> torch.Tensor:\n",
    "        # Simple min-max quantization\n",
    "        with torch.no_grad():\n",
    "            # Compute min and max along the specified axis\n",
    "            if axis == 0:\n",
    "                min_val = tensor.min(dim=0)[0]\n",
    "                max_val = tensor.max(dim=0)[0]\n",
    "            else:  # axis = -1\n",
    "                min_val = tensor.min(dim=-1, keepdim=True)[0]\n",
    "                max_val = tensor.max(dim=-1, keepdim=True)[0]\n",
    "            \n",
    "            # Scale to [0, 2^nbits - 1]\n",
    "            scale = (max_val - min_val) / (2**self.nbits - 1)\n",
    "            scale = torch.clamp(scale, min=1e-6)  # Prevent division by zero\n",
    "            \n",
    "            # Quantize\n",
    "            qtensor = ((tensor - min_val) / scale).round().clamp(0, 2**self.nbits - 1)\n",
    "            \n",
    "            # Store scaling factors as attributes of the tensor\n",
    "            qtensor.scale = scale\n",
    "            qtensor.zero_point = min_val\n",
    "            \n",
    "            return qtensor\n",
    "    \n",
    "    def _dequantize(self, qtensor: torch.Tensor) -> torch.Tensor:\n",
    "        # Dequantize using stored scale and zero point\n",
    "        with torch.no_grad():\n",
    "            return qtensor * qtensor.scale + qtensor.zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b94040-eda3-4011-a43d-22873df48de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPUOffloadedCache(DynamicCache):\n",
    "    \"\"\"Cache implementation that offloads to CPU while maintaining one layer on GPU\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.current_layer_idx = 0\n",
    "        self.device_tracking = []  # Track current device for each layer\n",
    "        self.original_devices = []  # Store original device for each layer\n",
    "        self._prefetch_stream = (\n",
    "            torch.cuda.Stream() if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Update cache with new states, managing device placement\"\"\"\n",
    "        # Track the original device when first seeing a layer\n",
    "        if layer_idx >= len(self.original_devices):\n",
    "            self.original_devices.append(key_states.device)\n",
    "            self.device_tracking.append(\"cpu\")  # Start on CPU except current layer\n",
    "            \n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "            \n",
    "        # Update cache\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            # New layer - store on original device initially\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        else:\n",
    "            # Existing layer - concatenate on current device\n",
    "            device = self.original_devices[layer_idx]\n",
    "            key_states = key_states.to(device)\n",
    "            value_states = value_states.to(device)\n",
    "            \n",
    "            self.key_cache[layer_idx] = torch.cat(\n",
    "                [self.key_cache[layer_idx], key_states], dim=-2\n",
    "            )\n",
    "            self.value_cache[layer_idx] = torch.cat(\n",
    "                [self.value_cache[layer_idx], value_states], dim=-2\n",
    "            )\n",
    "        \n",
    "        # Handle device management\n",
    "        self._manage_devices(layer_idx)\n",
    "        \n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "    \n",
    "    def _manage_devices(self, current_layer_idx: int):\n",
    "        \"\"\"Manage device placement of cache layers\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        # Move current layer to GPU if needed\n",
    "        if self.device_tracking[current_layer_idx] == \"cpu\":\n",
    "            self._move_to_device(\n",
    "                current_layer_idx, \n",
    "                self.original_devices[current_layer_idx]\n",
    "            )\n",
    "            \n",
    "        # Start prefetching next layer\n",
    "        next_layer_idx = (current_layer_idx + 1) % len(self.key_cache)\n",
    "        self._prefetch_layer(next_layer_idx)\n",
    "        \n",
    "        # Move previous layer to CPU\n",
    "        prev_layer_idx = (current_layer_idx - 1) % len(self.key_cache)\n",
    "        if prev_layer_idx != next_layer_idx:\n",
    "            self._offload_layer(prev_layer_idx)\n",
    "    \n",
    "    def _move_to_device(self, layer_idx: int, device: torch.device):\n",
    "        \"\"\"Move a layer to specified device\"\"\"\n",
    "        if self.device_tracking[layer_idx] == str(device):\n",
    "            return\n",
    "            \n",
    "        self.key_cache[layer_idx] = self.key_cache[layer_idx].to(device)\n",
    "        self.value_cache[layer_idx] = self.value_cache[layer_idx].to(device)\n",
    "        self.device_tracking[layer_idx] = str(device)\n",
    "    \n",
    "    def _prefetch_layer(self, layer_idx: int):\n",
    "        \"\"\"Prefetch next layer to GPU\"\"\"\n",
    "        if not self._prefetch_stream:\n",
    "            return\n",
    "            \n",
    "        with torch.cuda.stream(self._prefetch_stream):\n",
    "            self._move_to_device(\n",
    "                layer_idx,\n",
    "                self.original_devices[layer_idx]\n",
    "            )\n",
    "    \n",
    "    def _offload_layer(self, layer_idx: int):\n",
    "        \"\"\"Offload layer to CPU\"\"\"\n",
    "        if self.device_tracking[layer_idx] == \"cpu\":\n",
    "            return\n",
    "            \n",
    "        self.key_cache[layer_idx] = self.key_cache[layer_idx].to(\"cpu\")\n",
    "        self.value_cache[layer_idx] = self.value_cache[layer_idx].to(\"cpu\")\n",
    "        self.device_tracking[layer_idx] = \"cpu\"\n",
    "    \n",
    "    def get_current_device(self, layer_idx: int) -> str:\n",
    "        \"\"\"Get current device for layer\"\"\"\n",
    "        if layer_idx >= len(self.device_tracking):\n",
    "            return \"undefined\"\n",
    "        return self.device_tracking[layer_idx]\n",
    "    \n",
    "    def get_device_metrics(self) -> Dict[str, int]:\n",
    "        \"\"\"Get metrics about cache device placement\"\"\"\n",
    "        return {\n",
    "            'layers_on_gpu': sum(1 for d in self.device_tracking if d != \"cpu\"),\n",
    "            'layers_on_cpu': sum(1 for d in self.device_tracking if d == \"cpu\"),\n",
    "            'total_layers': len(self.device_tracking)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89b6701-fb8c-4867-90b6-0f11bace1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedOffloadedCache(SimpleQuantizedCache):\n",
    "    \"\"\"Cache implementation combining quantization with CPU offloading\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_config: QuantizedCacheConfig):\n",
    "        super().__init__(cache_config)\n",
    "        self.current_layer_idx = 0\n",
    "        self.device_tracking = []  # Track current device for each layer\n",
    "        self.original_devices = []  # Store original device for each layer\n",
    "        self._prefetch_stream = (\n",
    "            torch.cuda.Stream() if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Update cache with new states, managing both quantization and device placement\"\"\"\n",
    "        # Track the original device when first seeing a layer\n",
    "        if layer_idx >= len(self.original_devices):\n",
    "            self.original_devices.append(key_states.device)\n",
    "            self.device_tracking.append(\"cpu\")  # Start on CPU except current layer\n",
    "            \n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "            \n",
    "        # Update quantized cache\n",
    "        if len(self._quantized_key_cache) <= layer_idx:\n",
    "            # New layer - quantize and store\n",
    "            q_key = self._quantize(key_states.contiguous(), axis=self.axis_key)\n",
    "            q_value = self._quantize(value_states.contiguous(), axis=self.axis_value)\n",
    "            \n",
    "            # Store quantized data on CPU initially\n",
    "            self._quantized_key_cache.append(q_key.to(\"cpu\"))\n",
    "            self._quantized_value_cache.append(q_value.to(\"cpu\"))\n",
    "            \n",
    "            # Initialize residual cache\n",
    "            self.key_cache.append(torch.zeros(0, dtype=key_states.dtype, device=key_states.device))\n",
    "            self.value_cache.append(torch.zeros(0, dtype=value_states.dtype, device=value_states.device))\n",
    "            \n",
    "            keys_to_return, values_to_return = key_states, value_states\n",
    "        else:\n",
    "            # Handle existing layer\n",
    "            device = self.original_devices[layer_idx]\n",
    "            \n",
    "            # Move quantized data to correct device if needed\n",
    "            if self.device_tracking[layer_idx] != str(device):\n",
    "                self._quantized_key_cache[layer_idx] = self._quantized_key_cache[layer_idx].to(device)\n",
    "                self._quantized_value_cache[layer_idx] = self._quantized_value_cache[layer_idx].to(device)\n",
    "                self.device_tracking[layer_idx] = str(device)\n",
    "            \n",
    "            # Dequantize and combine with residual\n",
    "            dequant_key = self._dequantize(self._quantized_key_cache[layer_idx])\n",
    "            dequant_value = self._dequantize(self._quantized_value_cache[layer_idx])\n",
    "            \n",
    "            keys_to_return = [dequant_key, self.key_cache[layer_idx], key_states]\n",
    "            values_to_return = [dequant_value, self.value_cache[layer_idx], value_states]\n",
    "            \n",
    "            keys_to_return = torch.cat(keys_to_return, dim=-2)\n",
    "            values_to_return = torch.cat(values_to_return, dim=-2)\n",
    "            \n",
    "            # Check if we need to requantize and update residual\n",
    "            if (\n",
    "                self.key_cache[layer_idx].dim() == 4\n",
    "                and self.key_cache[layer_idx].shape[-2] + 1 >= self.residual_length\n",
    "            ):\n",
    "                # Requantize full sequence\n",
    "                self._quantized_key_cache[layer_idx] = self._quantize(\n",
    "                    keys_to_return.contiguous(), axis=self.axis_key\n",
    "                )\n",
    "                self._quantized_value_cache[layer_idx] = self._quantize(\n",
    "                    values_to_return.contiguous(), axis=self.axis_value\n",
    "                )\n",
    "                \n",
    "                # Move to CPU if not current layer\n",
    "                if layer_idx != self.current_layer_idx:\n",
    "                    self._quantized_key_cache[layer_idx] = self._quantized_key_cache[layer_idx].to(\"cpu\")\n",
    "                    self._quantized_value_cache[layer_idx] = self._quantized_value_cache[layer_idx].to(\"cpu\")\n",
    "                    self.device_tracking[layer_idx] = \"cpu\"\n",
    "                \n",
    "                # Reset residual\n",
    "                self.key_cache[layer_idx] = torch.zeros(0, dtype=key_states.dtype, device=key_states.device)\n",
    "                self.value_cache[layer_idx] = torch.zeros(0, dtype=value_states.dtype, device=value_states.device)\n",
    "            else:\n",
    "                # Update residual\n",
    "                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "        \n",
    "        # Handle device management\n",
    "        self._manage_devices(layer_idx)\n",
    "        \n",
    "        return keys_to_return, values_to_return\n",
    "    \n",
    "    def _manage_devices(self, current_layer_idx: int):\n",
    "        \"\"\"Manage device placement of cache layers\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        self.current_layer_idx = current_layer_idx\n",
    "        \n",
    "        # Move current layer to GPU if needed\n",
    "        if self.device_tracking[current_layer_idx] == \"cpu\":\n",
    "            self._move_to_device(\n",
    "                current_layer_idx, \n",
    "                self.original_devices[current_layer_idx]\n",
    "            )\n",
    "            \n",
    "        # Start prefetching next layer\n",
    "        next_layer_idx = (current_layer_idx + 1) % len(self._quantized_key_cache)\n",
    "        self._prefetch_layer(next_layer_idx)\n",
    "        \n",
    "        # Move previous layer to CPU\n",
    "        prev_layer_idx = (current_layer_idx - 1) % len(self._quantized_key_cache)\n",
    "        if prev_layer_idx != next_layer_idx:\n",
    "            self._offload_layer(prev_layer_idx)\n",
    "    \n",
    "    def _move_to_device(self, layer_idx: int, device: torch.device):\n",
    "        \"\"\"Move a layer to specified device\"\"\"\n",
    "        if self.device_tracking[layer_idx] == str(device):\n",
    "            return\n",
    "            \n",
    "        self._quantized_key_cache[layer_idx] = self._quantized_key_cache[layer_idx].to(device)\n",
    "        self._quantized_value_cache[layer_idx] = self._quantized_value_cache[layer_idx].to(device)\n",
    "        self.device_tracking[layer_idx] = str(device)\n",
    "    \n",
    "    def _prefetch_layer(self, layer_idx: int):\n",
    "        \"\"\"Prefetch next layer to GPU\"\"\"\n",
    "        if not self._prefetch_stream:\n",
    "            return\n",
    "            \n",
    "        with torch.cuda.stream(self._prefetch_stream):\n",
    "            self._move_to_device(\n",
    "                layer_idx,\n",
    "                self.original_devices[layer_idx]\n",
    "            )\n",
    "    \n",
    "    def _offload_layer(self, layer_idx: int):\n",
    "        \"\"\"Offload layer to CPU\"\"\"\n",
    "        if self.device_tracking[layer_idx] == \"cpu\":\n",
    "            return\n",
    "            \n",
    "        self._quantized_key_cache[layer_idx] = self._quantized_key_cache[layer_idx].to(\"cpu\")\n",
    "        self._quantized_value_cache[layer_idx] = self._quantized_value_cache[layer_idx].to(\"cpu\")\n",
    "        self.device_tracking[layer_idx] = \"cpu\"\n",
    "    \n",
    "    def get_device_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics about cache device placement and quantization\"\"\"\n",
    "        base_metrics = {\n",
    "            'layers_on_gpu': sum(1 for d in self.device_tracking if d != \"cpu\"),\n",
    "            'layers_on_cpu': sum(1 for d in self.device_tracking if d == \"cpu\"),\n",
    "            'total_layers': len(self.device_tracking)\n",
    "        }\n",
    "        \n",
    "        # Add quantization metrics\n",
    "        base_metrics.update({\n",
    "            'quantization_bits': self.nbits,\n",
    "            'residual_length': self.residual_length,\n",
    "            'compression_ratio': 32.0 / self.nbits  # Compared to FP32\n",
    "        })\n",
    "        \n",
    "        return base_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b5b55c-a21a-40d8-bc88-34a6ff47e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CacheConfig:\n",
    "    \"\"\"Configuration for cache strategies\"\"\"\n",
    "    strategy: str  # \"dynamic\", \"quantized\", \"cpu_offload\", or \"quantized_offload\"\n",
    "    decode_on_cpu: bool = False\n",
    "    quantization: Optional[Dict] = None\n",
    "    prefetch_size: int = 2  # Number of layers to prefetch for CPU offload\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cache(config: \"CacheConfig\", model_config) -> Optional[Cache]:\n",
    "        \"\"\"Initialize appropriate cache based on configuration\"\"\"\n",
    "        if config.strategy == \"dynamic\":\n",
    "            return DynamicCache()\n",
    "        elif config.strategy == \"quantized\":\n",
    "            if config.quantization is None:\n",
    "                config.quantization = {\n",
    "                    'nbits': 4,\n",
    "                    'residual_length': 128,\n",
    "                    'compute_dtype': torch.float16\n",
    "                }\n",
    "            quant_config = QuantizedCacheConfig(**config.quantization)\n",
    "            return SimpleQuantizedCache(cache_config=quant_config)\n",
    "        elif config.strategy == \"cpu_offload\":\n",
    "            return CPUOffloadedCache()\n",
    "        elif config.strategy == \"quantized_offload\":\n",
    "            if config.quantization is None:\n",
    "                config.quantization = {\n",
    "                    'nbits': 4,\n",
    "                    'residual_length': 128,\n",
    "                    'compute_dtype': torch.float16\n",
    "                }\n",
    "            quant_config = QuantizedCacheConfig(**config.quantization)\n",
    "            return QuantizedOffloadedCache(cache_config=quant_config)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b6f9d-b6cb-448e-9681-a814cb342ce5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f6b156-b331-46d1-afa0-f48f68726e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferencePhaseMetrics:\n",
    "    \"\"\"Detailed metrics for each inference phase (prefill/decode)\"\"\"\n",
    "    phase: str  # 'prefill' or 'decode'\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    tokens_processed: int\n",
    "    gpu_memory: Dict[str, float]  # Detailed memory stats\n",
    "    cpu_memory: Dict[str, float]\n",
    "    gpu_util: float\n",
    "    memory_bandwidth: float\n",
    "    cache_metrics: Dict[str, float]  # Cache-specific metrics\n",
    "\n",
    "@dataclass\n",
    "class TokenGenerationMetrics:\n",
    "    \"\"\"Enhanced per-token metrics\"\"\"\n",
    "    timestamp: float\n",
    "    token_index: int\n",
    "    phase: str\n",
    "    latency: float\n",
    "    gpu_memory: Dict[str, float]\n",
    "    cpu_memory: Dict[str, float]\n",
    "    gpu_util: float\n",
    "    cache_size: int\n",
    "    cache_metrics: Optional[Dict[str, float]] = None\n",
    "\n",
    "class EnhancedMetricsTracker:\n",
    "    \"\"\"Enhanced metrics tracking with phase awareness and detailed memory stats\"\"\"\n",
    "    def __init__(self, sampling_rate: float = 0.1):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.start_time = time.perf_counter()\n",
    "        self.current_phase = None\n",
    "        self.phase_metrics: List[InferencePhaseMetrics] = []\n",
    "        self.token_metrics: List[TokenGenerationMetrics] = []\n",
    "        \n",
    "        # Initialize CUDA events for memory bandwidth tracking\n",
    "        if torch.cuda.is_available():\n",
    "            self.start_event = torch.cuda.Event(enable_timing=True)\n",
    "            self.end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    def start_phase(self, phase: str):\n",
    "        \"\"\"Start tracking a new inference phase\"\"\"\n",
    "        self.current_phase = phase\n",
    "        self.start_event.record()\n",
    "        \n",
    "        self.phase_metrics.append(InferencePhaseMetrics(\n",
    "            phase=phase,\n",
    "            start_time=time.perf_counter(),\n",
    "            end_time=0,\n",
    "            tokens_processed=0,\n",
    "            gpu_memory=self._get_gpu_memory_stats(),\n",
    "            cpu_memory=self._get_cpu_memory_stats(),\n",
    "            gpu_util=self._get_gpu_utilization(),\n",
    "            memory_bandwidth=0,\n",
    "            cache_metrics={}\n",
    "        ))\n",
    "    \n",
    "    def end_phase(self, tokens_processed: int, cache_metrics: Optional[Dict[str, float]] = None):\n",
    "        \"\"\"End current phase and record final metrics\"\"\"\n",
    "        self.end_event.record()\n",
    "        self.end_event.synchronize()\n",
    "        \n",
    "        current_metrics = self.phase_metrics[-1]\n",
    "        current_metrics.end_time = time.perf_counter()\n",
    "        current_metrics.tokens_processed = tokens_processed\n",
    "        current_metrics.memory_bandwidth = self._calculate_memory_bandwidth()\n",
    "        if cache_metrics:\n",
    "            current_metrics.cache_metrics.update(cache_metrics)\n",
    "    \n",
    "    def sample_token(self, token_index: int, phase: str, latency: float, cache_size: int,\n",
    "                    cache_metrics: Optional[Dict[str, float]] = None):\n",
    "        \"\"\"Record metrics for a single token generation\"\"\"\n",
    "        self.token_metrics.append(TokenGenerationMetrics(\n",
    "            timestamp=time.perf_counter() - self.start_time,\n",
    "            token_index=token_index,\n",
    "            phase=phase,\n",
    "            latency=latency,\n",
    "            gpu_memory=self._get_gpu_memory_stats(),\n",
    "            cpu_memory=self._get_cpu_memory_stats(),\n",
    "            gpu_util=self._get_gpu_utilization(),\n",
    "            cache_size=cache_size,\n",
    "            cache_metrics=cache_metrics\n",
    "        ))\n",
    "    \n",
    "    def _get_gpu_memory_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get detailed GPU memory statistics\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {}\n",
    "            \n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / 1024**2,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1024**2,\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / 1024**2,\n",
    "            'max_reserved': torch.cuda.max_memory_reserved() / 1024**2,\n",
    "            'fragmentation': self._calculate_memory_fragmentation()\n",
    "        }\n",
    "    \n",
    "    def _get_cpu_memory_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Get detailed CPU memory statistics\"\"\"\n",
    "        process = psutil.Process()\n",
    "        return {\n",
    "            'rss': process.memory_info().rss / 1024**2,\n",
    "            'vms': process.memory_info().vms / 1024**2,\n",
    "            'shared': process.memory_info().shared / 1024**2,\n",
    "            'percent': process.memory_percent()\n",
    "        }\n",
    "    \n",
    "    def _calculate_memory_fragmentation(self) -> float:\n",
    "        \"\"\"Calculate memory fragmentation with improved accuracy\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0.0\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated()\n",
    "        reserved = torch.cuda.memory_reserved()\n",
    "        \n",
    "        if reserved == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Calculate fragmentation in MB\n",
    "        fragmentation = (reserved - allocated) / 1024**2\n",
    "        \n",
    "        return fragmentation\n",
    "    \n",
    "    def _get_gpu_utilization(self) -> float:\n",
    "        \"\"\"Get GPU utilization percentage with improved accuracy\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0.0\n",
    "        try:\n",
    "            # Try using nvidia-smi through subprocess if available\n",
    "            import subprocess\n",
    "            result = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            return float(result.strip()) / 100.0\n",
    "        except:\n",
    "            # Fallback to torch CUDA metrics\n",
    "            return torch.cuda.utilization() / 100.0\n",
    "    \n",
    "    def _calculate_memory_bandwidth(self) -> float:\n",
    "        \"\"\"Calculate memory bandwidth usage between events\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 0.0\n",
    "        return self.start_event.elapsed_time(self.end_event) * 1e-3  # Convert to seconds\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get enhanced statistical summary of collected metrics\"\"\"\n",
    "        # Debug print\n",
    "        logger.debug(f\"Number of token metrics collected: {len(self.token_metrics)}\")\n",
    "        logger.debug(f\"Phases recorded: {set(m.phase for m in self.token_metrics)}\")\n",
    "        \n",
    "        summary = {\n",
    "            'phases': {},\n",
    "            'memory': {\n",
    "                'peak_gpu_allocated': max(m.gpu_memory.get('allocated', 0) for m in self.token_metrics),\n",
    "                'peak_gpu_reserved': max(m.gpu_memory.get('reserved', 0) for m in self.token_metrics),\n",
    "                'peak_cpu': max(m.cpu_memory.get('rss', 0) for m in self.token_metrics),\n",
    "                'mean_fragmentation': np.mean([m.gpu_memory.get('fragmentation', 0) for m in self.token_metrics]),\n",
    "            },\n",
    "            'performance': {\n",
    "                'mean_latency': np.mean([m.latency for m in self.token_metrics]),\n",
    "                'p90_latency': np.percentile([m.latency for m in self.token_metrics], 90),\n",
    "                'mean_gpu_util': np.mean([m.gpu_util for m in self.token_metrics]),\n",
    "                'tokens_per_second': len(self.token_metrics) / (self.token_metrics[-1].timestamp if self.token_metrics else 1)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add phase-specific metrics\n",
    "        for phase in ['prefill', 'decode']:\n",
    "            phase_tokens = [m for m in self.token_metrics if m.phase == phase]\n",
    "            if phase_tokens:\n",
    "                summary['phases'][phase] = {\n",
    "                    'token_count': len(phase_tokens),\n",
    "                    'mean_latency': np.mean([m.latency for m in phase_tokens]),\n",
    "                    'peak_memory': max(m.gpu_memory.get('allocated', 0) for m in phase_tokens),\n",
    "                    'mean_cache_size': np.mean([m.cache_size for m in phase_tokens])\n",
    "                }\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd81e8-2472-4945-9b49-28ce67a5c9ec",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863583e3-fd4a-4ebc-8332-e573c4fc0df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2Inference:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        cache_config: Optional[CacheConfig] = None,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.cache_config = cache_config or CacheConfig(strategy=\"dynamic\")\n",
    "        self.metrics_tracker = None\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Initialize model with CPU decode offloading\"\"\"\n",
    "        logger.info(f\"Loading model '{self.model_name}'\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        if self.cache_config.decode_on_cpu:\n",
    "            # For Llama2, we need to access model.model.layers\n",
    "            if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
    "                logger.info(\"Moving decode layers to CPU...\")\n",
    "                \n",
    "                # Get number of layers\n",
    "                num_layers = len(self.model.model.layers)\n",
    "                \n",
    "                # Keep first few layers on GPU for better performance\n",
    "                gpu_layers = min(4, num_layers // 4)  # Keep ~25% on GPU\n",
    "                \n",
    "                # Move specific layers to CPU\n",
    "                for i in range(gpu_layers, num_layers):\n",
    "                    self.model.model.layers[i] = self.model.model.layers[i].to('cpu')\n",
    "                \n",
    "                logger.info(f\"Moved {num_layers - gpu_layers} layers to CPU, kept {gpu_layers} on GPU\")\n",
    "            else:\n",
    "                logger.warning(\"Could not find Llama2 layers for CPU offloading\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        # Initialize appropriate cache\n",
    "        self.cache = CacheConfig.get_cache(self.cache_config, self.model.config)\n",
    "        \n",
    "    def _prepare_inputs_for_cpu_decode(self, input_ids, attention_mask):\n",
    "        \"\"\"Prepare inputs for CPU decode phase\"\"\"\n",
    "        # Ensure proper device placement for CPU decode\n",
    "        if self.cache_config.decode_on_cpu:\n",
    "            # Move inputs to CPU but keep attention mask on GPU for efficiency\n",
    "            input_ids = input_ids.cpu()\n",
    "            # attention_mask stays on GPU as it's needed for attention computation\n",
    "            \n",
    "            # If using offloaded cache, ensure it's properly placed\n",
    "            if isinstance(self.cache, (CPUOffloadedCache, QuantizedOffloadedCache)):\n",
    "                self.cache.prepare_for_cpu_decode()\n",
    "        \n",
    "        return input_ids, attention_mask\n",
    "    \n",
    "    def _get_layer_device(self, layer_idx: int) -> str:\n",
    "        \"\"\"Get the device for a specific layer during decode\"\"\"\n",
    "        if not self.cache_config.decode_on_cpu:\n",
    "            return self.device\n",
    "            \n",
    "        # For Llama2 CPU offloading strategy:\n",
    "        # - Keep first few layers on GPU\n",
    "        # - Rest on CPU\n",
    "        num_layers = len(self.model.model.layers)\n",
    "        gpu_layers = min(4, num_layers // 4)\n",
    "        \n",
    "        return self.device if layer_idx < gpu_layers else \"cpu\"\n",
    "    \n",
    "    def _run_decode_step(self, input_ids, attention_mask, past_key_values):\n",
    "        \"\"\"Run a single decode step with CPU offloading\"\"\"\n",
    "        # Prepare inputs\n",
    "        input_ids, attention_mask = self._prepare_inputs_for_cpu_decode(\n",
    "            input_ids, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Run forward pass\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        # Move logits back to GPU for sampling\n",
    "        if self.cache_config.decode_on_cpu:\n",
    "            outputs.logits = outputs.logits.to(self.device)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5ab022-385f-4b51-b84b-52156df07afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    \"\"\"Configuration for benchmark runs\"\"\"\n",
    "    context_lengths: List[int] = (512, 2048, 4096)\n",
    "    output_length: int = 50\n",
    "    num_runs: int = 3  # Number of runs per configuration\n",
    "    warmup_runs: int = 1  # Number of warmup runs\n",
    "    model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    validate_outputs: bool = True\n",
    "    stress_test: bool = False\n",
    "\n",
    "class CacheBenchmark:\n",
    "    \"\"\"Enhanced benchmark suite for testing cache strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BenchmarkConfig):\n",
    "        self.config = config\n",
    "        self.strategies = self._get_default_strategies()\n",
    "        self.results = {}\n",
    "        self.validation_results = {}\n",
    "        \n",
    "    def _get_default_strategies(self) -> List[Dict]:\n",
    "        \"\"\"Define cache strategies to test\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"name\": \"dynamic\",\n",
    "                \"config\": CacheConfig(\n",
    "                    strategy=\"dynamic\",\n",
    "                    decode_on_cpu=False\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"quantized\",\n",
    "                \"config\": CacheConfig(\n",
    "                    strategy=\"quantized\",\n",
    "                    decode_on_cpu=False,\n",
    "                    quantization={\n",
    "                        'nbits': 4,\n",
    "                        'residual_length': 128,\n",
    "                        'compute_dtype': torch.float16\n",
    "                    }\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"cpu_offload\",\n",
    "                \"config\": CacheConfig(\n",
    "                    strategy=\"cpu_offload\",\n",
    "                    decode_on_cpu=True,\n",
    "                    prefetch_size=2\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"quantized_offload\",\n",
    "                \"config\": CacheConfig(\n",
    "                    strategy=\"quantized_offload\",\n",
    "                    decode_on_cpu=True,\n",
    "                    quantization={\n",
    "                        'nbits': 4,\n",
    "                        'residual_length': 128,\n",
    "                        'compute_dtype': torch.float16\n",
    "                    },\n",
    "                    prefetch_size=2\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def _validate_cache_behavior(\n",
    "        self,\n",
    "        llm: Llama2Inference,\n",
    "        strategy: Dict,\n",
    "        context_length: int\n",
    "    ) -> Dict[str, bool]:\n",
    "        \"\"\"Validate cache behavior for a given strategy\"\"\"\n",
    "        validation = {\n",
    "            'cache_growth_correct': False,\n",
    "            'device_placement_correct': False,\n",
    "            'memory_pattern_valid': False,\n",
    "            'quantization_active': False\n",
    "        }\n",
    "        \n",
    "        # Check cache growth\n",
    "        initial_length = llm.cache.get_seq_length()\n",
    "        _ = llm.run_inference(\"Test input\", max_new_tokens=5)\n",
    "        final_length = llm.cache.get_seq_length()\n",
    "        validation['cache_growth_correct'] = final_length > initial_length\n",
    "        \n",
    "        # Check device placement for offloading strategies\n",
    "        if strategy['config'].strategy in ['cpu_offload', 'quantized_offload']:\n",
    "            if hasattr(llm.cache, 'get_device_metrics'):\n",
    "                metrics = llm.cache.get_device_metrics()\n",
    "                validation['device_placement_correct'] = (\n",
    "                    metrics['layers_on_cpu'] > 0 and\n",
    "                    metrics['layers_on_gpu'] > 0\n",
    "                )\n",
    "        else:\n",
    "            validation['device_placement_correct'] = True\n",
    "        \n",
    "        # Check memory pattern\n",
    "        if torch.cuda.is_available():\n",
    "            initial_mem = torch.cuda.memory_allocated()\n",
    "            _ = llm.run_inference(\"Another test\", max_new_tokens=5)\n",
    "            peak_mem = torch.cuda.max_memory_allocated()\n",
    "            final_mem = torch.cuda.memory_allocated()\n",
    "            \n",
    "            # Memory should peak during generation but release after\n",
    "            validation['memory_pattern_valid'] = (\n",
    "                peak_mem > initial_mem and\n",
    "                final_mem < peak_mem\n",
    "            )\n",
    "        else:\n",
    "            validation['memory_pattern_valid'] = True\n",
    "        \n",
    "        # Check quantization\n",
    "        if strategy['config'].strategy in ['quantized', 'quantized_offload']:\n",
    "            if hasattr(llm.cache, 'get_device_metrics'):\n",
    "                metrics = llm.cache.get_device_metrics()\n",
    "                validation['quantization_active'] = (\n",
    "                    'quantization_bits' in metrics and\n",
    "                    'compression_ratio' in metrics\n",
    "                )\n",
    "        else:\n",
    "            validation['quantization_active'] = True\n",
    "            \n",
    "        return validation\n",
    "    \n",
    "    def _run_stress_test(\n",
    "        self,\n",
    "        llm: Llama2Inference,\n",
    "        context_length: int,\n",
    "        num_iterations: int = 5\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Run stress test with continuous generation\"\"\"\n",
    "        stress_metrics = {\n",
    "            'avg_throughput': 0.0,\n",
    "            'memory_growth': 0.0,\n",
    "            'device_transitions': 0\n",
    "        }\n",
    "        \n",
    "        initial_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "        throughputs = []\n",
    "        device_transitions = 0\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            start_time = time.perf_counter()\n",
    "            result = llm.run_inference(\n",
    "                f\"Test input {i}\",\n",
    "                max_new_tokens=50\n",
    "            )\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            # Calculate throughput\n",
    "            tokens_generated = len(result['decode_timings'])\n",
    "            throughput = tokens_generated / (end_time - start_time)\n",
    "            throughputs.append(throughput)\n",
    "            \n",
    "            # Track device transitions for offloading strategies\n",
    "            if hasattr(llm.cache, 'get_device_metrics'):\n",
    "                metrics = llm.cache.get_device_metrics()\n",
    "                device_transitions += metrics['layers_on_cpu']\n",
    "        \n",
    "        final_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "        \n",
    "        stress_metrics['avg_throughput'] = np.mean(throughputs)\n",
    "        stress_metrics['memory_growth'] = (final_mem - initial_mem) / 1024**2  # MB\n",
    "        stress_metrics['device_transitions'] = device_transitions\n",
    "        \n",
    "        return stress_metrics\n",
    "    \n",
    "    def run_benchmark(self) -> Dict:\n",
    "        \"\"\"Run complete benchmark suite\"\"\"\n",
    "        document_loader = EnhancedDocumentLoader()\n",
    "        \n",
    "        for context_length in self.config.context_lengths:\n",
    "            logger.info(f\"\\nTesting context length: {context_length}\")\n",
    "            self.results[context_length] = {}\n",
    "            self.validation_results[context_length] = {}\n",
    "            \n",
    "            # Load test input\n",
    "            input_text = document_loader.load_chunk(chunk_size=context_length)\n",
    "            \n",
    "            for strategy in self.strategies:\n",
    "                logger.info(f\"\\nTesting strategy: {strategy['name']}\")\n",
    "                strategy_results = []\n",
    "                \n",
    "                try:\n",
    "                    # Warmup runs\n",
    "                    logger.info(\"Performing warmup runs...\")\n",
    "                    llm = Llama2Inference(\n",
    "                        model_name=self.config.model_name,\n",
    "                        cache_config=strategy['config']\n",
    "                    )\n",
    "                    \n",
    "                    for _ in range(self.config.warmup_runs):\n",
    "                        _ = llm.run_inference(\n",
    "                            input_text=input_text,\n",
    "                            max_new_tokens=self.config.output_length\n",
    "                        )\n",
    "                    \n",
    "                    # Validation if requested\n",
    "                    if self.config.validate_outputs:\n",
    "                        logger.info(\"Validating cache behavior...\")\n",
    "                        self.validation_results[context_length][strategy['name']] = (\n",
    "                            self._validate_cache_behavior(llm, strategy, context_length)\n",
    "                        )\n",
    "                    \n",
    "                    # Benchmark runs\n",
    "                    logger.info(\"Starting benchmark runs...\")\n",
    "                    for run in range(self.config.num_runs):\n",
    "                        # Clear GPU memory\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                            torch.cuda.reset_peak_memory_stats()\n",
    "                        \n",
    "                        result = llm.run_inference(\n",
    "                            input_text=input_text,\n",
    "                            max_new_tokens=self.config.output_length\n",
    "                        )\n",
    "                        \n",
    "                        strategy_results.append(result)\n",
    "                        \n",
    "                        # Print progress metrics\n",
    "                        avg_token_time = np.mean([\n",
    "                            t['forward_latency'] \n",
    "                            for t in result['decode_timings']\n",
    "                        ])\n",
    "                        logger.info(\n",
    "                            f\"Run {run + 1}/{self.config.num_runs}: \"\n",
    "                            f\"avg token time = {avg_token_time*1000:.2f}ms\"\n",
    "                        )\n",
    "                    \n",
    "                    # Stress test if enabled\n",
    "                    if self.config.stress_test:\n",
    "                        logger.info(\"Running stress test...\")\n",
    "                        stress_metrics = self._run_stress_test(\n",
    "                            llm,\n",
    "                            context_length\n",
    "                        )\n",
    "                        self.results[context_length][f\"{strategy['name']}_stress\"] = (\n",
    "                            stress_metrics\n",
    "                        )\n",
    "                    \n",
    "                    # Aggregate results\n",
    "                    self.results[context_length][strategy['name']] = {\n",
    "                        'avg_token_time': np.mean([\n",
    "                            np.mean([t['forward_latency'] for t in r['decode_timings']])\n",
    "                            for r in strategy_results\n",
    "                        ]),\n",
    "                        'throughput': np.mean([\n",
    "                            len(r['decode_timings']) / sum(\n",
    "                                t['forward_latency'] for t in r['decode_timings']\n",
    "                            )\n",
    "                            for r in strategy_results\n",
    "                        ]),\n",
    "                        'peak_gpu_memory': max([\n",
    "                            r['metrics']['memory']['peak_gpu_allocated']\n",
    "                            for r in strategy_results\n",
    "                        ]) if torch.cuda.is_available() else 0,\n",
    "                        'peak_cpu_memory': max([\n",
    "                            r['metrics']['memory']['peak_cpu']\n",
    "                            for r in strategy_results\n",
    "                        ]),\n",
    "                        'cache_metrics': strategy_results[-1]['metrics'].get(\n",
    "                            'cache_metrics', {}\n",
    "                        )\n",
    "                    }\n",
    "                    \n",
    "                    # Cleanup\n",
    "                    llm.cleanup()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error testing {strategy['name']}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print formatted summary of benchmark results\"\"\"\n",
    "        print(\"\\nBenchmark Summary:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for context_length, strategies in self.results.items():\n",
    "            print(f\"\\nContext Length: {context_length}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Print main metrics\n",
    "            headers = [\"Strategy\", \"Throughput\", \"GPU Mem\", \"CPU Mem\", \"Avg Token\"]\n",
    "            row_format = \"{:15} {:>10} {:>10} {:>10} {:>10}\"\n",
    "            print(row_format.format(*headers))\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for strategy, metrics in strategies.items():\n",
    "                if not strategy.endswith('_stress'):\n",
    "                    print(row_format.format(\n",
    "                        strategy,\n",
    "                        f\"{metrics['throughput']:.1f}\",\n",
    "                        f\"{metrics['peak_gpu_memory']:.0f}MB\",\n",
    "                        f\"{metrics['peak_cpu_memory']:.0f}MB\",\n",
    "                        f\"{metrics['avg_token_time']*1000:.1f}ms\"\n",
    "                    ))\n",
    "            \n",
    "            # Print validation results if available\n",
    "            if context_length in self.validation_results:\n",
    "                print(\"\\nValidation Results:\")\n",
    "                for strategy, validation in self.validation_results[context_length].items():\n",
    "                    print(f\"\\n{strategy}:\")\n",
    "                    for check, passed in validation.items():\n",
    "                        print(f\"  {check}: {'' if passed else ''}\")\n",
    "            \n",
    "            # Print stress test results if available\n",
    "            stress_results = {\n",
    "                k: v for k, v in strategies.items() if k.endswith('_stress')\n",
    "            }\n",
    "            if stress_results:\n",
    "                print(\"\\nStress Test Results:\")\n",
    "                for strategy, metrics in stress_results.items():\n",
    "                    base_strategy = strategy.replace('_stress', '')\n",
    "                    print(f\"\\n{base_strategy}:\")\n",
    "                    print(f\"  Avg Throughput: {metrics['avg_throughput']:.1f} tokens/s\")\n",
    "                    print(f\"  Memory Growth: {metrics['memory_growth']:.0f}MB\")\n",
    "                    print(f\"  Device Transitions: {metrics['device_transitions']}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "#     # Configure benchmark\n",
    "#     config = BenchmarkConfig(\n",
    "#         context_lengths=[512, 2048, 4096],\n",
    "#         output_length=50,\n",
    "#         num_runs=3,\n",
    "#         warmup_runs=1,\n",
    "#         validate_outputs=True,\n",
    "#         stress_test=True\n",
    "#     )\n",
    "    \n",
    "#     # Run benchmark\n",
    "#     benchmark = CacheBenchmark(config)\n",
    "#     results = benchmark.run_benchmark()\n",
    "    \n",
    "#     # Print results\n",
    "#     benchmark.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04443d62-5531-4900-b098-1372a379ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test with minimal configuration first\n",
    "# config = BenchmarkConfig(\n",
    "#     context_lengths=[512],  # Start small\n",
    "#     output_length=20,\n",
    "#     num_runs=1,\n",
    "#     warmup_runs=1,\n",
    "#     validate_outputs=True,\n",
    "#     stress_test=False\n",
    "# )\n",
    "\n",
    "# # Initialize and run benchmark\n",
    "# benchmark = CacheBenchmark(config)\n",
    "# results = benchmark.run_benchmark()\n",
    "# benchmark.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c22f77b-6563-4115-b40f-606c88a97dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Llama2 with CPU offloading...\n",
      "INFO:__main__:Loading model 'meta-llama/Llama-2-7b-chat-hf'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4188aa96a5c24222bd40b2de2c512185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd7f06535dd42f4a4bb873b869a0cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956a486b038f48eb95c64d897e09d28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f3921b9ace423d8099310fb9187ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc80acbf9694285949604bc6062354b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2fcf7e885d42569caa4a827744b973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb2b5da7a214eea8ffa55ed38825ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Moving decode layers to CPU...\n",
      "INFO:__main__:Moved 28 layers to CPU, kept 4 on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83e05ade14e4235a85ef6ee7055b3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ce749044e74d87895d3af76641bcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d79c4be5734bff9f9c43bee00a3088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6009939a1db42d1bdde4bfef1464d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Checking layer device placement:\n",
      "INFO:__main__:Layer 0: cuda:0\n",
      "INFO:__main__:Layer 1: cuda:0\n",
      "INFO:__main__:Layer 2: cuda:0\n",
      "INFO:__main__:Layer 3: cuda:0\n",
      "INFO:__main__:Layer 4: cpu\n",
      "INFO:__main__:Layer 5: cpu\n",
      "INFO:__main__:Layer 6: cpu\n",
      "INFO:__main__:Layer 7: cpu\n",
      "INFO:__main__:Layer 8: cpu\n",
      "INFO:__main__:Layer 9: cpu\n",
      "INFO:__main__:Layer 10: cpu\n",
      "INFO:__main__:Layer 11: cpu\n",
      "INFO:__main__:Layer 12: cpu\n",
      "INFO:__main__:Layer 13: cpu\n",
      "INFO:__main__:Layer 14: cpu\n",
      "INFO:__main__:Layer 15: cpu\n",
      "INFO:__main__:Layer 16: cpu\n",
      "INFO:__main__:Layer 17: cpu\n",
      "INFO:__main__:Layer 18: cpu\n",
      "INFO:__main__:Layer 19: cpu\n",
      "INFO:__main__:Layer 20: cpu\n",
      "INFO:__main__:Layer 21: cpu\n",
      "INFO:__main__:Layer 22: cpu\n",
      "INFO:__main__:Layer 23: cpu\n",
      "INFO:__main__:Layer 24: cpu\n",
      "INFO:__main__:Layer 25: cpu\n",
      "INFO:__main__:Layer 26: cpu\n",
      "INFO:__main__:Layer 27: cpu\n",
      "INFO:__main__:Layer 28: cpu\n",
      "INFO:__main__:Layer 29: cpu\n",
      "INFO:__main__:Layer 30: cpu\n",
      "INFO:__main__:Layer 31: cpu\n",
      "INFO:__main__:\n",
      "Running inference...\n",
      "ERROR:__main__:Test failed: 'CPUOffloadedCache' object has no attribute 'prepare_for_cpu_decode'\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/jbi4001_11130465/ipykernel_119012/2962637224.py\", line 67, in <module>\n",
      "    test_llama2_inference()\n",
      "  File \"/scratch/jbi4001_11130465/ipykernel_119012/2962637224.py\", line 38, in test_llama2_inference\n",
      "    outputs = llm._run_decode_step(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/jbi4001_11130465/ipykernel_119012/2730220608.py\", line 80, in _run_decode_step\n",
      "    input_ids, attention_mask = self._prepare_inputs_for_cpu_decode(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/jbi4001_11130465/ipykernel_119012/2730220608.py\", line 60, in _prepare_inputs_for_cpu_decode\n",
      "    self.cache.prepare_for_cpu_decode()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jbi4001/miniforge3/envs/proteomics/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1709, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'CPUOffloadedCache' object has no attribute 'prepare_for_cpu_decode'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def test_llama2_inference():\n",
    "    \"\"\"Simple test of Llama2 inference with CPU offloading\"\"\"\n",
    "    \n",
    "    # Initialize with CPU offloading\n",
    "    logger.info(\"Initializing Llama2 with CPU offloading...\")\n",
    "    llm = Llama2Inference(\n",
    "        model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        cache_config=CacheConfig(strategy=\"cpu_offload\", decode_on_cpu=True)\n",
    "    )\n",
    "    llm.setup()\n",
    "    \n",
    "    # Test input\n",
    "    test_prompt = \"Write a short story about a cat in 2-3 sentences.\"\n",
    "    \n",
    "    # Check layer devices\n",
    "    logger.info(\"\\nChecking layer device placement:\")\n",
    "    for i, layer in enumerate(llm.model.model.layers):\n",
    "        device = next(layer.parameters()).device\n",
    "        logger.info(f\"Layer {i}: {device}\")\n",
    "    \n",
    "    # Run inference\n",
    "    logger.info(\"\\nRunning inference...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = llm.tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(llm.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate with minimal tokens for testing\n",
    "    outputs = llm._run_decode_step(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        past_key_values=None\n",
    "    )\n",
    "    \n",
    "    # Test generation of a few tokens\n",
    "    for _ in range(3):  # Generate 3 tokens for testing\n",
    "        next_token = torch.argmax(outputs.logits[:, -1:], dim=-1)\n",
    "        device = next_token.device\n",
    "        logger.info(f\"Generated token device: {device}\")\n",
    "        \n",
    "        outputs = llm._run_decode_step(\n",
    "            input_ids=next_token,\n",
    "            attention_mask=torch.ones((1, next_token.shape[1]), device=llm.device),\n",
    "            past_key_values=outputs.past_key_values\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Inference time: {end_time - start_time:.2f}s\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del llm.model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        test_llama2_inference()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Test failed: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75551ede-9bb7-4bb3-b24d-af9bd4f1db27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
