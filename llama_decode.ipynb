{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427c425-b082-4578-8116-0e026dc3666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load HF token from .env\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f87c606-e892-463a-b8ed-a3b22b9fd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "decode_on_cpu = True\n",
    "max_new_tokens = 50\n",
    "input_text = \"This is a test prompt. The model should continue this text with a meaningful completion.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b19c5-bc25-4d54-a4c1-2f9064a7c49f",
   "metadata": {},
   "source": [
    "#################################################################\n",
    "# Helper Classes and Methods\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b9b849-1113-41fc-999a-44dbe4526058",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PhaseMetrics:\n",
    "    phase: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    tokens_processed: int\n",
    "\n",
    "@dataclass\n",
    "class TokenMetrics:\n",
    "    token_index: int\n",
    "    latency: float\n",
    "    cpu_memory_mb: float\n",
    "\n",
    "class EnhancedMetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.phases = {}\n",
    "        self.token_metrics = []\n",
    "        self.start_time = time.perf_counter()\n",
    "\n",
    "    def start_phase(self, phase: str):\n",
    "        self.phases[phase] = PhaseMetrics(phase, time.perf_counter(), None, 0)\n",
    "\n",
    "    def end_phase(self, phase: str, tokens_processed: int):\n",
    "        self.phases[phase].end_time = time.perf_counter()\n",
    "        self.phases[phase].tokens_processed = tokens_processed\n",
    "\n",
    "    def sample_token(self, token_index: int, latency: float):\n",
    "        # CPU memory usage\n",
    "        process = psutil.Process()\n",
    "        mem_info = process.memory_info().rss / (1024**2)\n",
    "        self.token_metrics.append(TokenMetrics(\n",
    "            token_index=token_index,\n",
    "            latency=latency,\n",
    "            cpu_memory_mb=mem_info\n",
    "        ))\n",
    "\n",
    "    def get_summary(self):\n",
    "        summary = {}\n",
    "        for phase, pm in self.phases.items():\n",
    "            duration = pm.end_time - pm.start_time if pm.end_time else 0\n",
    "            summary[phase] = {\n",
    "                'duration_sec': duration,\n",
    "                'tokens_processed': pm.tokens_processed,\n",
    "                'tokens_per_sec': pm.tokens_processed / duration if duration > 0 else 0.0\n",
    "            }\n",
    "\n",
    "        # Overall token stats\n",
    "        if self.token_metrics:\n",
    "            latencies = [t.latency for t in self.token_metrics]\n",
    "            mem_usages = [t.cpu_memory_mb for t in self.token_metrics]\n",
    "            summary['token_stats'] = {\n",
    "                'mean_latency_sec': np.mean(latencies),\n",
    "                'p90_latency_sec': np.percentile(latencies, 90),\n",
    "                'peak_cpu_memory_mb': max(mem_usages),\n",
    "                'final_cpu_memory_mb': mem_usages[-1]\n",
    "            }\n",
    "        else:\n",
    "            summary['token_stats'] = {}\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9ae97-0199-4b21-ba85-b6a931b7fac2",
   "metadata": {},
   "source": [
    "#################################################################\n",
    "# Model Setup\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b099c119-5f3b-44fe-8b4e-555f02177f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer on CPU directly\n",
    "# Using `device_map='cpu'` ensures the model and weights load into CPU memory.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cpu',\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model = model.float()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144f33c-d08e-4c06-aced-c2e21221cbae",
   "metadata": {},
   "source": [
    "#################################################################\n",
    "# Inference on CPU\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c6624b-7f29-418e-8d58-0725c50af034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize on CPU\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "metrics_tracker = EnhancedMetricsTracker()\n",
    "\n",
    "# Prefill phase\n",
    "metrics_tracker.start_phase('prefill')\n",
    "\n",
    "prefill_start = time.perf_counter()\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs, use_cache=True)\n",
    "prefill_end = time.perf_counter()\n",
    "\n",
    "metrics_tracker.end_phase('prefill', tokens_processed=inputs[\"input_ids\"].shape[1])\n",
    "\n",
    "# Decode phase\n",
    "metrics_tracker.start_phase('decode')\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "for i in range(max_new_tokens):\n",
    "    token_start = time.perf_counter()\n",
    "    # Get next token logits from last output\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    # Greedy sampling\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "    # Append next token to input_ids and attention_mask\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones((1,1), dtype=attention_mask.dtype)], dim=-1)\n",
    "\n",
    "    # Forward pass for next step\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=True)\n",
    "\n",
    "    token_end = time.perf_counter()\n",
    "    token_latency = token_end - token_start\n",
    "    metrics_tracker.sample_token(token_index=i, latency=token_latency)\n",
    "\n",
    "    # Check for EOS\n",
    "    if next_token.item() == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "metrics_tracker.end_phase('decode', tokens_processed=i+1)\n",
    "\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf2f98-b287-48c1-9ff4-37b44751a2d3",
   "metadata": {},
   "source": [
    "#################################################################\n",
    "# Results\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ac4c3a-c4cc-4e04-a926-ab513d618217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: This is a test prompt. The model should continue this text with a meaningful completion.\n",
      "Metrics Summary: {'prefill': {'duration_sec': 0.6922823674976826, 'tokens_processed': 18, 'tokens_per_sec': 26.00095112210157}, 'decode': {'duration_sec': 0.6258094608783722, 'tokens_processed': 1, 'tokens_per_sec': 1.5979304604893994}, 'token_stats': {'mean_latency_sec': 0.6247733123600483, 'p90_latency_sec': 0.6247733123600483, 'peak_cpu_memory_mb': 5515.12890625, 'final_cpu_memory_mb': 5515.12890625}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "summary = metrics_tracker.get_summary()\n",
    "print(\"Metrics Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91637b1c-37a8-413b-848a-33d5e606fc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
